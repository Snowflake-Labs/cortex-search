{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "cell1",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Multi-hop RAG Preprocessing for Complex PDFs\n",
    "\n",
    "This notebook walks you through building a multi-hop RAG engine using Snowflake. Multi-hop RAG addresses limitations of traditional RAG by combining semantic vector search with intelligent document graph traversal. The system performs two-step retrieval: first using vector search to find relevant documents, then traversing connected documents through extracted references to provide rich augmented context. This approach is essential for comprehensive responses to complex inquiries, particularly in technical domains with interconnected information.\n",
    "\n",
    "We'll create a multimodal Cortex Search service for document hybrid retrieval, analyze documents to extract cross-references, build a knowledge graph of connected pages, and develop SQL functions for intelligent document traversal. This infrastructure will power a Streamlit application for multimodal multi-hop RAG.\n",
    "\n",
    "## Building on Multimodal RAG Foundation\n",
    "\n",
    "**Important Note**: This notebook builds upon the [multimodal RAG pipeline](https://github.com/Snowflake-Labs/cortex-search/blob/main/examples/08_multimodal_rag/cortex_search_multimodal.ipynb) foundation. While multihop RAG can work with various document modalities, multimodal retrieval provides significantly enhanced accuracy for our target queries and content: **operating and maintenance manuals**. These manuals typically contain: \n",
    "- **Complex text** with technical terminology and cross-references\n",
    "- **Tabular data** with specifications, part numbers, and procedures\n",
    "- **Visual elements** including diagrams, charts, and technical illustrations\n",
    "- **Mixed content** where text, tables, and images are deeply interconnected\n",
    "\n",
    "## Overview\n",
    "The multihop RAG preprocessing involves:\n",
    "1. **Document Parsing**: Extract text from PDFs and generated multimodal vector embeddings using `PARSE_DOCUMENT` and `AI_EMBED` SQL functions\n",
    "2. **Vector DB Setup**: Build multimodal and hybrid (semantic and lexical) search with Cortex Search\n",
    "3. **Document Analysis**: Extract page cross-references using `AI_COMPLETE` SQL function\n",
    "4. **Graph DB Setup**: Build simple edge table representing page connections, and a recursive SQL function for easy document traversal\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "> **üìù Note**: If you've already completed the [multimodal RAG](https://github.com/Snowflake-Labs/cortex-search/blob/main/examples/08_multimodal_rag/cortex_search_multimodal.ipynb) notebook, you can proceed directly using your processed files.\n",
    "\n",
    "In this notebook, we use a [sample maintenance manual](https://www.graco.com/content/dam/graco/tech_documents/manuals/312/312796/312796EN-S.pdf) that has already been processed through [multimodal RAG](https://github.com/Snowflake-Labs/cortex-search/blob/main/examples/08_multimodal_rag/cortex_search_multimodal.ipynb) notebook. That PDF manual has been split into single-page PDFs and converted to corresponding PNG images.\n",
    "\n",
    "üìÅ **Download Files**: [Sample Processed Documents](https://drive.google.com/drive/folders/1OKfqpAts2cXkDZ3ZwosagTJvH3cBcmbu?usp=sharing)\n",
    "\n",
    "After downloading, upload the files to your Snowflake internal stage maintaining the directory structure:\n",
    "- `raw_pdf/` - Original PDF document \n",
    "- `paged_pdf/` - PDF files for each page e.g. `{document_name}_page_{page_number}.pdf`\n",
    "- `paged_image/` - PNG images of each page e.g. `{document_name}_page_{page_number}.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b0cf9a-c0a7-400a-b473-2b7c64bda3d4",
   "metadata": {
    "collapsed": false,
    "name": "cell2",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's set some required parameters to specify which documents needs be processed from your internal stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd4059-8026-4fc5-9393-502df7d9691d",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Configuration - Update these based on your setup\n",
    "SOURCE_DOCS_STAGE = \"@CORTEX_SEARCH_DOCS.DATA.DOCS\"\n",
    "SOURCE_DOCS_PATH = \"raw_pdf\"\n",
    "SOURCE_DOCS_PDF_FILTER = \"paged_pdf/%.pdf\"  # Use to filter paged PDFs for specific document(s)\n",
    "SOURCE_DOCS_PNG_FILTER = \"paged_image/%.png\"  # Use to filter paged images for specific document(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabbdb92-38d3-404d-bb73-09e7855b375e",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "let's set session context, namely the database and schema, to be used in subsequent SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a545f4-e522-4be2-80c5-98101ee93790",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "USE DATABASE CORTEX_SEARCH_DOCS;\n",
    "USE SCHEMA DATA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00dccb-0f50-404a-9a21-6d2c3f609ec4",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# Import required libraries for analysis steps\n",
    "import pandas as pd\n",
    "import json\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "print(f\"Connected to: {session.get_current_database()}.{session.get_current_schema()}\")\n",
    "\n",
    "# Count documents by querying stage directly\n",
    "count_query = f\"\"\"\n",
    "SELECT COUNT(*) as document_count\n",
    "FROM DIRECTORY('{SOURCE_DOCS_STAGE}')\n",
    "WHERE RELATIVE_PATH LIKE '{SOURCE_DOCS_PDF_FILTER}'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    document_count = session.sql(count_query).collect()[0]['DOCUMENT_COUNT']\n",
    "    print(f\"üìÑ Found {document_count} documents matching pattern: {SOURCE_DOCS_PDF_FILTER}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error counting documents: {str(e)}\")\n",
    "    print(f\"   Stage: {SOURCE_DOCS_STAGE}\")\n",
    "    print(f\"   Filter: {SOURCE_DOCS_PDF_FILTER}\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278aa3e-18d5-4044-b8eb-ab69b2e48518",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "## Step 2: Document Parsing - Extract Texts and Embed Images\n",
    "\n",
    "> **üìù Note**: Skip this section if you've already completed [multimodal RAG](https://github.com/Snowflake-Labs/cortex-search/blob/main/examples/08_multimodal_rag/cortex_search_multimodal.ipynb) notebook.\n",
    "\n",
    "Let's use Snowflake's `PARSE_DOCUMENT` to extract text from all PDFs. We'll later analyze the text to identify and extract cross-references between pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0ea96-5b8b-48c1-98f7-07f9621a2162",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCS_PDF_PARSED AS\n",
    "SELECT\n",
    "    RELATIVE_PATH AS FILE_NAME,\n",
    "    REGEXP_SUBSTR(RELATIVE_PATH, '_page_([0-9]+)\\.', 1, 1, 'e') as PAGE_NUMBER,\n",
    "    PARSE_JSON(TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "        '{{SOURCE_DOCS_STAGE}}',\n",
    "        RELATIVE_PATH,\n",
    "        {'mode': 'LAYOUT'}\n",
    "    ))):content AS PARSE_DOC_OUTPUT\n",
    "FROM DIRECTORY('{{SOURCE_DOCS_STAGE}}')\n",
    "WHERE\n",
    "    RELATIVE_PATH LIKE '{{SOURCE_DOCS_PDF_FILTER}}';\n",
    "\n",
    "SELECT * FROM DOCS_PDF_PARSED LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee08f6-b62c-4810-ba17-f400bcb64351",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "Let's use Snowflake's `AI_EMBED` to create image vector embeddings for all PNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25931cdb-2533-4107-acb7-30658ed727c3",
   "metadata": {
    "language": "sql",
    "name": "cell16",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCS_IMAGE_VECTORS AS\n",
    "SELECT\n",
    "    RELATIVE_PATH AS FILE_NAME,\n",
    "    REGEXP_SUBSTR(RELATIVE_PATH, '_page_([0-9]+)\\.', 1, 1, 'e') as PAGE_NUMBER,\n",
    "    AI_EMBED('voyage-multimodal-3', TO_FILE ('{{SOURCE_DOCS_STAGE}}', RELATIVE_PATH)) AS IMAGE_VECTOR,\n",
    "FROM DIRECTORY('{{SOURCE_DOCS_STAGE}}')\n",
    "WHERE\n",
    "    RELATIVE_PATH LIKE '{{SOURCE_DOCS_PNG_FILTER}}';\n",
    "\n",
    "SELECT * FROM DOCS_IMAGE_VECTORS LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e4267-b766-46a3-b285-91a5239c3410",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "Let's join the image vectors and parsed texts into a single table, and create a Cortex Search service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f7216-8ba2-4f50-817b-6131c1ffa2d6",
   "metadata": {
    "language": "sql",
    "name": "cell26",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCS_JOINED_DATA AS\n",
    "SELECT\n",
    "    v.FILE_NAME AS IMAGE_FILEPATH,\n",
    "    v.PAGE_NUMBER,\n",
    "    v.IMAGE_VECTOR AS VECTOR_MAIN,\n",
    "    p.PARSE_DOC_OUTPUT AS TEXT\n",
    "FROM\n",
    "    DOCS_IMAGE_VECTORS v\n",
    "JOIN\n",
    "    DOCS_PDF_PARSED p\n",
    "ON\n",
    "    SPLIT_PART(SPLIT_PART(v.FILE_NAME, '/', -1), '.', 1) = SPLIT_PART(SPLIT_PART(p.FILE_NAME, '/', -1), '.', 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da020659-9d5c-4e31-b818-3df11063a203",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "Let's create a Cortex Search service with a text index over the textual data (for lexical matching), and a vector index over the vector embeddings (for semantic matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d19cbf-22eb-4f27-9067-b74ba43ce0d1",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# First, set the warehouse that Cortex Search uses to materialize results when base table changes\n",
    "CORTEX_SEARCH_WH = 'COMPUTE_WH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd3ec0-3e76-44de-b862-012042831ade",
   "metadata": {
    "language": "sql",
    "name": "cell29",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE DOCS_SEARCH_MULTIMODAL\n",
    "  TEXT INDEXES TEXT\n",
    "  VECTOR INDEXES VECTOR_MAIN\n",
    "  ATTRIBUTES PAGE_NUMBER\n",
    "  WAREHOUSE='{{CORTEX_SEARCH_WH}}'\n",
    "  TARGET_LAG='1 day'\n",
    "AS (\n",
    "    SELECT \n",
    "        TO_VARCHAR(TEXT) AS TEXT, \n",
    "        PAGE_NUMBER, \n",
    "        VECTOR_MAIN,\n",
    "        IMAGE_FILEPATH\n",
    "    FROM DOCS_JOINED_DATA\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31301a0f-fa2e-45c4-b91f-e713c4d926ae",
   "metadata": {
    "name": "cell30"
   },
   "source": [
    "Cortex Search `DOCS_SEARCH_MULTIMODAL` is both hybrid and multimodal:\n",
    "- Hybrid: keyword (lexical) and vector (semantic) similiarity search\n",
    "- Multimodal: search across text and images\n",
    "\n",
    "Congratulations! You have implemented a multimodal retrieval using Cortex Search service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "cell4",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Document Analysis - Extract Page References\n",
    "\n",
    "This step uses AI to analyze each document page and extract references to other pages or external links. This is the foundation of our document graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cdcb8-8738-47d6-b64a-07b0a1a3ed70",
   "metadata": {
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TRANSIENT TABLE DOCS_ANALYSIS AS\n",
    "SELECT \n",
    "    IMAGE_FILEPATH,\n",
    "    PAGE_NUMBER,\n",
    "    TEXT as page_text,\n",
    "    AI_COMPLETE(\n",
    "        model => 'claude-3-7-sonnet',\n",
    "        prompt => CONCAT(\n",
    "            'You are a document analysis assistant. For the following text, identify and extract all references to other documents, pages or external links. Examples include: \"See page X for...\", \"See procedure to the left\", \"See Fig. X\", \"Refer to table X\", \"See volume Y\".\n",
    "            \n",
    "            Return a JSON array where each element is a JSON object with the following structure: \n",
    "\n",
    "            {\"type\": \"doc_reference\"|\"page_reference\"|\"external_link\",\n",
    "             \"value\": \"the inferred reference (e.g., document name, external URL or page number including current page)\", \n",
    "             \"context\": \"the full sentence or phrase containing the reference\", \n",
    "             \"explanation\": \"brief explanation of what the reference points to\"}.\n",
    "\n",
    "            Rules to follow:\n",
    "            - External link reference value must be a string representing a URL or URI or relative path.\n",
    "            - Page reference value must be numeric. Exclude strings like \"page\", \"current page\" or \"next page\" from value field.\n",
    "            - A self-reference is a reference to a table, figure, section, side, or paragraph in the same text. A self-reference should have type set to `page_reference` and value set to the current page number: ', PAGE_NUMBER, '.\n",
    "            - Do not consider part or spec numbers as page references, unless the text is referencing those as pages or documents.\n",
    "            - Do not include the page number in the page header or footer as a reference.\n",
    "            - Do not return duplicate references.\n",
    "\n",
    "            Text to analyze: ', TEXT\n",
    "        ),\n",
    "        response_format => {\n",
    "            'type':'json',\n",
    "            'schema':{\n",
    "                'type' : 'object',\n",
    "                'properties' : {\n",
    "                    'references':{\n",
    "                        'type':'array',\n",
    "                        'items':{\n",
    "                            'type':'object',\n",
    "                            'properties':{\n",
    "                                'type' : {'type':'string','enum': ['document_reference', 'page_reference', 'external_link']},\n",
    "                                'value': {'type':'string'},\n",
    "                                'context': {'type':'string'},\n",
    "                                'explanation': {'type':'string'}\n",
    "                            },\n",
    "                            'required':['type','value' ,'context','explanation']\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            } \n",
    "        },\n",
    "        model_parameters => {\n",
    "            'temperature': 0\n",
    "        }\n",
    "    ) as llm_response\n",
    "FROM DOCS_PDF_PARSED\n",
    "WHERE IMAGE_FILEPATH LIKE '{{SOURCE_DOCS_PDF_FILTER}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd8f91-9ce8-47b3-96cd-0a431981e87d",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "# Check the results of the document analysis\n",
    "analysis_count = session.sql(\"SELECT COUNT(*) as count FROM DOCS_ANALYSIS\").to_pandas()\n",
    "print(f\"‚úÖ Document analysis completed!\")\n",
    "print(f\"Analyzed {analysis_count['COUNT'].iloc[0]} document pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d028ad0-b0bb-4aa8-927f-dc4b65faeaf7",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "Let's display the AI-extracted results for some of the pages (8 through 10). We flatten the references array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ec63c-7550-4888-8b51-01890b7d6585",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    t.IMAGE_FILEPATH as FILE_NAME,\n",
    "    t.PAGE_NUMBER,\n",
    "    reference.value:type::STRING as reference_type,\n",
    "    reference.value:value::STRING as reference_value,\n",
    "    reference.value:context::STRING as reference_context,\n",
    "    reference.value:explanation::STRING as reference_explanation,\n",
    "    t.llm_response as raw_llm_response\n",
    "FROM DOCS_ANALYSIS t,\n",
    "LATERAL FLATTEN(input => t.llm_response:references) reference\n",
    "WHERE\n",
    "    t.llm_response IS NOT NULL\n",
    "    AND t.PAGE_NUMBER IN (8, 9, 10)\n",
    "ORDER BY \n",
    "    t.PAGE_NUMBER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "name": "cell7",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Build Document Graph - Create Edges Table\n",
    "\n",
    "Now we'll create the edges table that represents connections between documents. This is the core of our multihop capability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9539a7-62f1-4d00-b3a6-f22add0d2b95",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCS_EDGES (\n",
    "    SRC_DOC_FILEPATH STRING,       -- Source document path\n",
    "    SRC_PAGE_NUMBER NUMBER,        -- Source page number\n",
    "    DST_DOC_FILEPATH STRING,       -- Destination document path (derived from reference)\n",
    "    DST_PAGE_NUMBER NUMBER,        -- Destination page number (if page reference)\n",
    "    REFERENCE_TYPE STRING,         -- 'page_reference' or 'external_link'\n",
    "    REFERENCE_CONTEXT STRING,      -- The context where reference appears\n",
    "    REFERENCE_EXPLANATION STRING,  -- AI explanation of the reference\n",
    "    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "language": "sql",
    "name": "cell9",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO DOCS_EDGES (\n",
    "    SRC_DOC_FILEPATH,\n",
    "    SRC_PAGE_NUMBER,\n",
    "    DST_DOC_FILEPATH,\n",
    "    DST_PAGE_NUMBER,\n",
    "    REFERENCE_TYPE,\n",
    "    REFERENCE_CONTEXT,\n",
    "    REFERENCE_EXPLANATION\n",
    ")\n",
    "SELECT \n",
    "    t.IMAGE_FILEPATH as SRC_DOC_FILEPATH,\n",
    "    t.PAGE_NUMBER as SRC_PAGE_NUMBER,\n",
    "    -- For page references, construct the destination file path\n",
    "    CASE \n",
    "        WHEN reference.value:type::STRING = 'page_reference' \n",
    "        THEN REGEXP_REPLACE(t.IMAGE_FILEPATH, '(_page_\\\\d+)', '_page_' || reference.value:value::STRING)\n",
    "        ELSE reference.value:value::STRING  -- For external links, store the URL\n",
    "    END as DST_DOC_FILEPATH,\n",
    "    -- Extract page number if it's a page reference\n",
    "    CASE \n",
    "        WHEN reference.value:type::STRING = 'page_reference' \n",
    "        THEN TRY_CAST(reference.value:value::STRING AS NUMBER)\n",
    "        ELSE NULL\n",
    "    END as DST_PAGE_NUMBER,\n",
    "    reference.value:type::STRING as REFERENCE_TYPE,\n",
    "    reference.value:context::STRING as REFERENCE_CONTEXT,\n",
    "    reference.value:explanation::STRING as REFERENCE_EXPLANATION\n",
    "FROM DOCS_ANALYSIS t,\n",
    "LATERAL FLATTEN(input => t.llm_response:references) reference\n",
    "WHERE\n",
    "    t.llm_response IS NOT NULL\n",
    "    AND reference.value:type::STRING IS NOT NULL\n",
    "    AND reference.value:value::STRING IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9f4a1-e8ff-49e7-8d9c-d0dcc79f0ae6",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "SELECT REFERENCE_TYPE, COUNT(*) as count \n",
    "FROM DOCS_EDGES \n",
    "GROUP BY REFERENCE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "name": "cell11",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Create the FIND_CONNECTED_PAGES Function\n",
    "\n",
    "This is the core function that will be used by our Streamlit app for multihop document retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce6bbb-eb12-4c67-bfe8-5c38815d0e68",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION FIND_CONNECTED_PAGES(\n",
    "    start_paths ARRAY,\n",
    "    max_hops INTEGER\n",
    ")\n",
    "RETURNS TABLE (\n",
    "    source_path STRING,\n",
    "    source_page NUMBER,\n",
    "    dest_path STRING,\n",
    "    dest_page NUMBER,\n",
    "    ref_type STRING,\n",
    "    ref_count INTEGER,\n",
    "    explanations ARRAY,\n",
    "    contexts ARRAY,\n",
    "    hop_count INTEGER\n",
    ")\n",
    "AS\n",
    "$$\n",
    "    WITH RECURSIVE connected_images AS (\n",
    "        -- Base case: start with our initial set of images\n",
    "        SELECT \n",
    "            e.*,\n",
    "            0 as hop_count\n",
    "        FROM DOCS_EDGES e\n",
    "        WHERE ARRAY_CONTAINS(e.SRC_DOC_FILEPATH::VARIANT, START_PATHS)\n",
    "          AND e.REFERENCE_TYPE = 'page_reference'\n",
    "          AND e.DST_PAGE_NUMBER IS NOT NULL\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Recursive case: find images connected to our current set\n",
    "        SELECT \n",
    "            e.*,\n",
    "            c.hop_count + 1\n",
    "        FROM DOCS_EDGES e\n",
    "        JOIN connected_images c \n",
    "            ON e.SRC_DOC_FILEPATH = c.DST_DOC_FILEPATH\n",
    "        WHERE e.REFERENCE_TYPE = 'page_reference'\n",
    "          AND e.DST_PAGE_NUMBER IS NOT NULL\n",
    "          AND c.hop_count < MAX_HOPS\n",
    "    )\n",
    "    SELECT \n",
    "        SRC_DOC_FILEPATH as source_path,\n",
    "        SRC_PAGE_NUMBER as source_page,\n",
    "        DST_DOC_FILEPATH as dest_path,\n",
    "        DST_PAGE_NUMBER as dest_page,\n",
    "        REFERENCE_TYPE as ref_type,\n",
    "        COUNT(*) as ref_count,\n",
    "        ARRAY_AGG(DISTINCT REFERENCE_EXPLANATION) as explanations,\n",
    "        ARRAY_AGG(DISTINCT REFERENCE_CONTEXT) as contexts,\n",
    "        MIN(hop_count) as hop_count\n",
    "    FROM connected_images\n",
    "    GROUP BY 1, 2, 3, 4, 5\n",
    "    ORDER BY hop_count, ref_count DESC\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "name": "cell13",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Test the Function & Final Validation\n",
    "\n",
    "Let's test our function with the same format that will be used by the Streamlit app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a7411-db16-495d-9d8b-de317a800a0b",
   "metadata": {
    "language": "sql",
    "name": "step6_test",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Test the function with sample image paths\n",
    "SELECT\n",
    "    DEST_PATH AS dest_path,\n",
    "    DEST_PAGE AS dest_page,\n",
    "    ARRAY_AGG(exp.value) AS explanations\n",
    "FROM TABLE(FIND_CONNECTED_PAGES(\n",
    "    ARRAY_CONSTRUCT('paged_image/312796EN-S_page_8.png', 'paged_image/312796EN-S_page_13.png'),\n",
    "    2\n",
    ")),\n",
    "LATERAL FLATTEN(input => EXPLANATIONS) exp\n",
    "GROUP BY 1, 2\n",
    "ORDER BY DEST_PAGE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba311de8-e23c-4b7f-9e5f-3be820c78c59",
   "metadata": {
    "language": "python",
    "name": "step6_test_results"
   },
   "outputs": [],
   "source": [
    "function_results = step6_test.to_pandas()\n",
    "print(f\"üß™ Function Test Results:\")\n",
    "print(f\"Found {len(function_results)} connected documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "sql",
    "name": "step6_verification",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Final verification that all required objects exist\n",
    "\n",
    "-- Check Cortex Search Service by describing the specific service\n",
    "DESC CORTEX SEARCH SERVICE DOCS_SEARCH_MULTIMODAL;\n",
    "\n",
    "SELECT 'DOCS_SEARCH_MULTIMODAL' as object_name,\n",
    "    'CORTEX SEARCH' as object_type,\n",
    "    CASE WHEN COUNT(*) > 0 THEN '‚úÖ EXISTS' ELSE '‚ùå MISSING' END as status\n",
    "FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'DOCS_ANALYSIS' as object_name,\n",
    "    'TABLE' as object_type,\n",
    "    CASE WHEN COUNT(*) > 0 THEN '‚úÖ EXISTS' ELSE '‚ùå MISSING' END as status\n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE TABLE_SCHEMA = CURRENT_SCHEMA() AND TABLE_NAME = 'DOCS_ANALYSIS'\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'DOCS_EDGES' as object_name,\n",
    "    'TABLE' as object_type,\n",
    "    CASE WHEN COUNT(*) > 0 THEN '‚úÖ EXISTS' ELSE '‚ùå MISSING' END as status\n",
    "FROM INFORMATION_SCHEMA.TABLES\n",
    "WHERE TABLE_SCHEMA = CURRENT_SCHEMA() AND TABLE_NAME = 'DOCS_EDGES'\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'FIND_CONNECTED_PAGES' as object_name,\n",
    "    'FUNCTION' as object_type,\n",
    "    CASE WHEN COUNT(*) > 0 THEN '‚úÖ EXISTS' ELSE '‚ùå MISSING' END as status\n",
    "FROM INFORMATION_SCHEMA.FUNCTIONS\n",
    "WHERE FUNCTION_SCHEMA = CURRENT_SCHEMA() AND FUNCTION_NAME = 'FIND_CONNECTED_PAGES'\n",
    "\n",
    "ORDER BY object_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258b564-7d2e-494f-8569-89e7ac44c894",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "verification_df = step6_verification.to_pandas()\n",
    "\n",
    "# Check if all objects exist\n",
    "all_exist = all(verification_df['STATUS'].str.contains('EXISTS'))\n",
    "if all_exist:\n",
    "    print(\"\\nüéâ SUCCESS! All required objects have been created.\")\n",
    "    print(\"Your multihop RAG preprocessing is complete!\")\n",
    "    print(\"\\nüì± You can now use the Streamlit app: streamlit_chatbot_multihop_rag.py\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some objects are missing. Please review the errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "cell18",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "üéØ **What We've Built:**\n",
    "\n",
    "1. **Document Parsing Tables** (`DOCS_PDF_PARSED`, `DOCS_IMAGE_VECTORS`): Extracted text content and generated image vectors from document pages using Voyage AI multimodal embedding model\n",
    "2. **Unified Data Table** (`DOCS_JOINED_DATA`): Combined text and vector data for each document page\n",
    "3. **Cortex Search Service** (`DOCS_SEARCH_MULTIMODAL`): Hybrid multimodal search service with both text and vector indexes for semantic and lexical retrieval\n",
    "4. **Document Analysis Table** (`DOCS_ANALYSIS`): AI-extracted references and cross-references from each document page using Claude 3.7 Sonnet \n",
    "5. **Document Graph** (`DOCS_EDGES`): Connections between document pages based on extracted references, forming a knowledge graph\n",
    "6. **Traversal Function** (`FIND_CONNECTED_PAGES`): Recursive SQL function for graph-like page traversal to enable multihop document discovery\n",
    "7. **Complete Infrastructure**: End-to-end pipeline from raw documents to searchable, connected, and traversable knowledge base\n",
    "\n",
    "üöÄ **Next Steps:**\n",
    "\n",
    "1. **Deploy the Streamlit App**: Use `streamlit_chatbot_multihop_rag.py` to deploy a multihop RAG application that combines vector search and graph traversal\n",
    "2. **Test Queries**: Ask questions that require information from multiple connected pages to see multihop retrieval in action\n",
    "3. **Monitor Performance**: Use the debug mode to see how documents are being retrieved and traversed through the graph\n",
    "4. **Iterate**: Add more documents, adjust the hop count, or refine the AI reference extraction prompts based on your use case\n",
    "\n",
    "Your multihop multimodal RAG system is now ready to provide intelligent responses using rich augmented context from both **hybrid vector search** and **document graph traversal**! üéâ\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "roy.arsan@snowflake.com",
   "authorId": "3533595235718",
   "authorName": "RARSAN",
   "lastEditTime": 1752646675187,
   "notebookId": "ad75p2bko5gvzr2wupzi",
   "sessionId": "c4adf36c-cc4c-458f-b69c-35468620ab10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
