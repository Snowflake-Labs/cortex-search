{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "xw7ke2yiifgrxgxsyy2f",
   "authorId": "7316233358469",
   "authorName": "PYU",
   "authorEmail": "puxuan.yu@snowflake.com",
   "sessionId": "e3312463-a075-4b08-a68c-4eb1627ec394",
   "lastEditTime": 1743811030949
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02b90c5-e06b-4082-ad0c-7e3a5e31a457",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "# Implementing multimodal retrieval using Cortex Search Service\n\nWelcome! This tutorial shows a lightweight example where a customer has 2 long pdfs and wants to search and ask natural questions on them. On a high level, this tutorial demonstrates:\n\n- Convert long PDF files to document screenshots (images).\n- (Optional but highly recommended) Run parse_document on PDFs for auxiliary text retrieval to further improve quality.\n- Embed document screenshots using EMBED_IMAGE_1024 (PrPr) which runs `voyage-multimodal-3` under the hood\n- Create a Cortex Search Service using multimodal embeddings and OCR text.\n- Retrieve top pages using Cortex Search.\n- Get natural language answer with multimodal RAG!"
  },
  {
   "cell_type": "markdown",
   "id": "30728acc-9579-4149-bcc5-b3241e5eac65",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "To start with, make sure you have PDFs stored under a stage. The two PDF files used in this demo can be found ![here](https://drive.google.com/drive/folders/1bExhPiJlF9aNushnXeLLBR4m9EMaShHw?usp=sharing)."
  },
  {
   "cell_type": "code",
   "id": "505524cb-873d-425d-83e9-de8ec433b5e4",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- CREATE SCHEMA IF NOT EXISTS CORTEX_SEARCH_DB.PYU;\n-- CREATE OR REPLACE STAGE CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO\n-- STORAGE_INTEGRATION = ML_DEV\n-- URL = 's3://ml-dev-sfc-or-dev-misc1-k8s/cortexsearch/pyu/multimodal/demo/'\n-- DIRECTORY = (ENABLE = TRUE);\n\n-- CREATE OR REPLACE STAGE CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL DIRECTORY = (ENABLE = TRUE) ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');\n\n-- COPY FILES INTO @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\n-- FROM @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO/raw_pdf/;\n\nLS @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab710021-6fc0-4358-88eb-4bac717184c3",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Now let's run some python code:\n\nThe purpose is to paginate raw pages into pages -- in image and PDF format. Images are for multimodal retrieval, while PDFs are for better OCR quality (optional). As long as you configure the config correctly, you are good to go!\n\n```\nclass Config:\n    input_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n    output_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/\"\n    input_path: str = \"raw_pdf\"\n    output_pdf_path: str = \"paged_pdf\"\n    output_image_path: str = \"paged_image\"\n    allowed_extensions: List[str] = None\n    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n    dpi: int = 300  # Default DPI for image conversion\n\n    def __post_init__(self):\n        if self.allowed_extensions is None:\n            self.allowed_extensions = [\".pdf\"]\n```\n\n**Make sure the output_stage is an internal stage**, because `embed_image_1024` only works with internal stages at the moment."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport os\nimport sys\nimport tempfile\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import List\nfrom typing import Tuple\n\nimport pdfplumber\nimport PyPDF2\nimport snowflake.snowpark.session as session\nimport streamlit as st\n\n\ndef print_info(msg: str) -> None:\n    \"\"\"Print info message\"\"\"\n    print(f\"INFO: {msg}\", file=sys.stderr)\n\n\ndef print_error(msg: str) -> None:\n    \"\"\"Print error message\"\"\"\n    print(f\"ERROR: {msg}\", file=sys.stderr)\n    if hasattr(st, \"error\"):\n        st.error(msg)\n\n\ndef print_warning(msg: str) -> None:\n    \"\"\"Print warning message\"\"\"\n    print(f\"WARNING: {msg}\", file=sys.stderr)\n\n\n@dataclass\nclass Config:\n    input_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n    output_stage: str = (\n        \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/\"  # Base output stage without subdirectories\n    )\n    input_path: str = \"raw_pdf\"\n    output_pdf_path: str = \"paged_pdf\"\n    output_image_path: str = \"paged_image\"\n    allowed_extensions: List[str] = None\n    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n    dpi: int = 300  # Default DPI for image conversion\n\n    def __post_init__(self):\n        if self.allowed_extensions is None:\n            self.allowed_extensions = [\".pdf\"]\n\n\nclass PDFProcessingError(Exception):\n    \"\"\"Base exception for PDF processing errors\"\"\"\n\n\nclass FileDownloadError(PDFProcessingError):\n    \"\"\"Raised when file download fails\"\"\"\n\n\nclass PDFConversionError(PDFProcessingError):\n    \"\"\"Raised when PDF conversion fails\"\"\"\n\n\n@contextmanager\ndef managed_temp_file(suffix: str = None) -> str:\n    \"\"\"Context manager for temporary file handling\"\"\"\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n    try:\n        yield temp_file.name\n    finally:\n        # Don't delete the file immediately, let the caller handle cleanup\n        pass\n\n\ndef cleanup_temp_file(file_path: str) -> None:\n    \"\"\"Clean up a temporary file\"\"\"\n    try:\n        if os.path.exists(file_path):\n            os.unlink(file_path)\n    except OSError as e:\n        print_warning(f\"Failed to delete temporary file {file_path}: {e}\")\n\n\ndef list_pdf_files(session: session.Session, config: Config) -> List[dict]:\n    \"\"\"List all PDF files in the source stage\"\"\"\n    try:\n        # Use LIST command instead of DIRECTORY function\n        query = f\"\"\"\n        LIST {config.input_stage}\n        \"\"\"\n\n        file_list = session.sql(query).collect()\n\n        # Filter for PDF files\n        pdf_files = []\n        for file_info in file_list:\n            full_path = file_info[\"name\"]\n            # Extract just the filename from the full path\n            file_name = os.path.basename(full_path)\n\n            if any(\n                file_name.lower().endswith(ext) for ext in config.allowed_extensions\n            ):\n                pdf_files.append(\n                    {\n                        \"RELATIVE_PATH\": file_name,  # Use just the filename\n                        \"SIZE\": file_info[\"size\"] if \"size\" in file_info else 0,\n                    }\n                )\n\n        print_info(f\"Found {len(pdf_files)} PDF files in the stage\")\n        return pdf_files\n    except Exception as e:\n        print_error(f\"Failed to list files: {e}\")\n        raise\n\n\ndef download_file_from_stage(\n    session: session.Session, file_path: str, config: Config\n) -> str:\n    \"\"\"Download a file from stage using session.file.get\"\"\"\n    # Create a temporary directory\n    temp_dir = tempfile.mkdtemp()\n    try:\n        # Ensure there are no double slashes in the path\n        stage_path = f\"{config.input_stage.rstrip('/')}/{file_path.lstrip('/')}\"\n\n        # Get the file from stage\n        get_result = session.file.get(stage_path, temp_dir)\n        if not get_result or get_result[0].status != \"DOWNLOADED\":\n            raise FileDownloadError(f\"Failed to download file: {file_path}\")\n\n        # Construct the local path where the file was downloaded\n        local_path = os.path.join(temp_dir, os.path.basename(file_path))\n        if not os.path.exists(local_path):\n            raise FileDownloadError(f\"Downloaded file not found at: {local_path}\")\n\n        return local_path\n    except Exception as e:\n        print_error(f\"Error downloading {file_path}: {e}\")\n        # Clean up the temporary directory\n        try:\n            import shutil\n\n            shutil.rmtree(temp_dir)\n        except Exception as cleanup_error:\n            print_warning(f\"Failed to clean up temporary directory: {cleanup_error}\")\n        raise FileDownloadError(f\"Failed to download file: {e}\")\n\n\ndef convert_pdf_to_images(pdf_path: str, config: Config) -> List[Tuple[str, int]]:\n    \"\"\"Convert PDF pages to images\"\"\"\n    temp_files = []  # Keep track of temporary files for cleanup\n    try:\n        # Open PDF with pdfplumber\n        with pdfplumber.open(pdf_path) as pdf:\n            print_info(f\"Converting PDF to {len(pdf.pages)} images\")\n\n            segments = []\n            for i, page in enumerate(pdf.pages):\n                # Get page dimensions\n                width = page.width\n                height = page.height\n\n                # Determine if scaling is needed\n                max_dim = max(width, height)\n                if max_dim > config.max_dimension:\n                    # Calculate scale factor to fit within max_dimension\n                    scale_factor = config.max_dimension / max_dim\n                    width = int(width * scale_factor)\n                    height = int(height * scale_factor)\n\n                # Create temporary file for the image\n                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n                temp_files.append(temp_file.name)\n\n                # Convert page to image with specified DPI\n                img = page.to_image(resolution=config.dpi)\n                img.save(temp_file.name)\n\n                segments.append((temp_file.name, i + 1))\n\n            return segments\n    except Exception as e:\n        print_error(f\"Error converting PDF to images: {e}\")\n        # Clean up any temporary files created so far\n        for temp_file in temp_files:\n            cleanup_temp_file(temp_file)\n        raise PDFConversionError(f\"Failed to convert PDF to images: {e}\")\n\n\ndef extract_pdf_pages(pdf_path: str, config: Config) -> List[Tuple[str, int]]:\n    \"\"\"Extract individual pages from PDF using PyPDF2\"\"\"\n    temp_files = []  # Keep track of temporary files for cleanup\n    try:\n        # Open PDF with PyPDF2\n        with open(pdf_path, \"rb\") as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            num_pages = len(pdf_reader.pages)\n            print_info(f\"Extracting {num_pages} pages from PDF\")\n\n            segments = []\n            for i in range(num_pages):\n                # Create temporary file for the page\n                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n                temp_files.append(temp_file.name)\n\n                # Create a new PDF with just this page\n                pdf_writer = PyPDF2.PdfWriter()\n                pdf_writer.add_page(pdf_reader.pages[i])\n\n                # Save the page to the temporary file\n                with open(temp_file.name, \"wb\") as output_file:\n                    pdf_writer.write(output_file)\n\n                segments.append((temp_file.name, i + 1))\n\n            return segments\n    except Exception as e:\n        print_error(f\"Error extracting PDF pages: {e}\")\n        # Clean up any temporary files created so far\n        for temp_file in temp_files:\n            cleanup_temp_file(temp_file)\n        raise PDFConversionError(f\"Failed to extract PDF pages: {e}\")\n\n\ndef upload_file_to_stage(\n    session: session.Session, file_path: str, output_path: str, config: Config\n) -> str:\n    \"\"\"Upload file to the output stage\"\"\"\n    try:\n        # Get the directory and filename from the output path\n        output_dir = os.path.dirname(output_path)\n        base_name = os.path.basename(output_path)\n\n        # Create the full stage path with subdirectory\n        stage_path = f\"{config.output_stage.rstrip('/')}/{output_dir.lstrip('/')}\"\n\n        # Read the content of the original file\n        with open(file_path, \"rb\") as f:\n            file_content = f.read()\n\n        # Create a new file with the correct name\n        temp_dir = tempfile.gettempdir()\n        temp_file_path = os.path.join(temp_dir, base_name)\n\n        # Write the content to the new file\n        with open(temp_file_path, \"wb\") as f:\n            f.write(file_content)\n\n        # Upload the file using session.file.put with compression disabled\n        put_result = session.file.put(\n            temp_file_path, stage_path, auto_compress=False, overwrite=True\n        )\n\n        # Check upload status\n        if not put_result or len(put_result) == 0:\n            raise Exception(f\"Failed to upload file: {base_name}\")\n\n        if put_result[0].status not in [\"UPLOADED\", \"SKIPPED\"]:\n            raise Exception(f\"Upload failed with status: {put_result[0].status}\")\n\n        # Clean up the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n\n        return f\"Successfully uploaded {base_name} to {stage_path}\"\n    except Exception as e:\n        print_error(f\"Error uploading file: {e}\")\n        raise\n\n\ndef process_pdf_files(config: Config) -> None:\n    \"\"\"Main process to orchestrate the PDF splitting\"\"\"\n    try:\n        session = get_active_session()\n        pdf_files = list_pdf_files(session, config)\n\n        for file_info in pdf_files:\n            file_path = file_info[\"RELATIVE_PATH\"]\n            print_info(f\"Processing: {file_path}\")\n\n            try:\n                # Download the PDF file\n                local_pdf_path = download_file_from_stage(session, file_path, config)\n\n                # Get base filename without extension\n                base_name = os.path.splitext(os.path.basename(file_path))[0]\n\n                # Extract individual PDF pages\n                pdf_segments = extract_pdf_pages(local_pdf_path, config)\n\n                # Convert PDF to images\n                image_segments = convert_pdf_to_images(local_pdf_path, config)\n\n                # Process each page\n                for (pdf_segment, page_num), (image_segment, _) in zip(\n                    pdf_segments, image_segments\n                ):\n                    try:\n                        # Create proper output paths with correct naming\n                        pdf_output_path = (\n                            f\"{config.output_pdf_path}/{base_name}_page_{page_num}.pdf\"\n                        )\n                        image_output_path = f\"{config.output_image_path}/{base_name}_page_{page_num}.png\"\n\n                        # Upload PDF page directly from the temporary file\n                        upload_file_to_stage(\n                            session, pdf_segment, pdf_output_path, config\n                        )\n\n                        # Upload image page directly from the temporary file\n                        upload_file_to_stage(\n                            session, image_segment, image_output_path, config\n                        )\n                    except Exception as e:\n                        print_error(\n                            f\"Error processing page {page_num} of {file_path}: {e}\"\n                        )\n                    finally:\n                        # Clean up temporary files\n                        cleanup_temp_file(pdf_segment)\n                        cleanup_temp_file(image_segment)\n\n                # Clean up the original downloaded file\n                cleanup_temp_file(local_pdf_path)\n\n            except Exception as e:\n                print_error(f\"Error processing {file_path}: {e}\")\n                continue\n\n    except Exception as e:\n        print_error(f\"Fatal error in process_pdf_files: {e}\")\n        raise\n\n\n\nconfig = Config(dpi=200)\nprocess_pdf_files(config)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c8857644-dc84-4c19-b0a4-594d5e19586c",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Now let's start the multimodal embedding part! We first create an intermediate table that holds relative file names of images, and then call `SNOWFLAKE.CORTEX.embed_image_1024` to turn them into vectors!"
  },
  {
   "cell_type": "code",
   "id": "bd6854a8-8f72-45e3-936a-8b6b6f792bf8",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS AS\nSELECT\n    CONCAT('paged_image/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\nFROM\n    @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_image/\nGROUP BY FILE_NAME, STAGE_PREFIX\n;\n\nSELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcc56ebb-45a4-4ded-b9ed-2d11b6054af1",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS AS\nSELECT FILE_NAME, STAGE_PREFIX, SNOWFLAKE.CORTEX.embed_image_1024('voyage-multimodal-3', '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL', FILE_NAME) AS IMAGE_VECTOR\nFROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS;\n\n\nSELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90ce0184-ea75-415c-9911-c70028a9a20b",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "Similarly, we call `SNOWFLAKE.CORTEX.PARSE_DOCUMENT` to extract text from PDF pages. We discover that, although multimodal retrieval is powerful, augmenting it with text retrieval for keyword matching can bring quality improvement on certain types of search tasks/queries."
  },
  {
   "cell_type": "code",
   "id": "b002eaac-2163-4f96-8a77-9c052e4381db",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_PDF_CORPUS AS\nSELECT\n    CONCAT('paged_pdf/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\nFROM\n    @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_pdf/\nGROUP BY FILE_NAME, STAGE_PREFIX\n;\n\nCREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC AS\n    SELECT\n        FILE_NAME,\n        STAGE_PREFIX,\n        PARSE_JSON(TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n            '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL',\n            FILE_NAME,\n            {'mode': 'LAYOUT'}\n        ))):content AS PARSE_DOC_OUTPUT\n    FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_PDF_CORPUS\n;\n\nSELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41e715c5-ccef-40dc-958c-c655d780f457",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "Now we join image vectors and texts into a single table, and create a Cortex Search service!"
  },
  {
   "cell_type": "code",
   "id": "fd6901c9-6563-46ff-8369-9234169a799a",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_JOINED_DATA AS\nSELECT\n    REGEXP_SUBSTR(v.FILE_NAME, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) AS INCLUDE_PAGE_ID,\n    v.IMAGE_VECTOR AS VECTOR_MAIN,\n    p.PARSE_DOC_OUTPUT AS TEXT\nFROM\n    CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS v\nJOIN\n    CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC p\nON\n    REGEXP_SUBSTR(v.FILE_NAME, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) = REGEXP_SUBSTR(p.FILE_NAME, 'paged_pdf/(.*)\\\\.pdf$', 1, 1, 'e', 1);\n\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE CORTEX_SEARCH_DB.PYU.DEMO_SEC_CORTEX_SEARCH_SERVICE\n  TEXT INDEXES TEXT\n  VECTOR INDEXES VECTOR_MAIN\n  WAREHOUSE='SEARCH_L'\n  TARGET_LAG='1 day'\nAS (\n    SELECT \n        TO_VARCHAR(TEXT) AS TEXT, \n        INCLUDE_PAGE_ID, \n        VECTOR_MAIN\n    FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_JOINED_DATA\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78771232-88b1-4096-83fb-b4e233e548d8",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "Note that multimodal retrieval is not GA in cortex search, thus we cannot specify a multimodal embedding model when creating the service. Instead, we will embed queries directly with `SNOWFLAKE.CORTEX.EMBED_TEXT_1024` and call cortex search service with `experimental={'queryEmbedding': query_vector}`"
  },
  {
   "cell_type": "code",
   "id": "f5611514-3ef0-45e9-a921-28eabe710893",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\ndemo_query_text = \"What was the overall operational cost incurred by Abbott Laboratories in 2023, and how much of this amount was allocated to research and development?\"\nsql_output = session.sql(f\"\"\"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_1024('voyage-multimodal-3', 'Represent the query for retrieving supporting documents:  {demo_query_text}')\"\"\").collect()\nquery_vector = list(sql_output[0].asDict().values())[0]\nprint(query_vector)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7e908c4-aa91-40d2-943b-5f2934741373",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.core import Root\n\n\nroot = Root(session)\n# fetch service\nmy_service = (root\n  .databases[\"CORTEX_SEARCH_DB\"]\n  .schemas[\"PYU\"]\n  .cortex_search_services[\"DEMO_SEC_CORTEX_SEARCH_SERVICE\"]\n)\n\n# query service\nresp = my_service.search(\n  query=demo_query_text,\n  columns=[\"TEXT\", \"INCLUDE_PAGE_ID\"],\n  limit=5,\n  experimental={'queryEmbedding': query_vector}\n)\n\nfor i in range(5):\n    print(f\"rank {i + 1}: {resp.to_dict()['results'][i]['INCLUDE_PAGE_ID']}\")\n\ntop_page_id = resp.to_dict()['results'][0]['INCLUDE_PAGE_ID']",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f448324b-c09d-4ed2-b449-94b0615b124b",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "Let's see the top ranked page we found!"
  },
  {
   "cell_type": "code",
   "id": "93407e70-4657-472e-a6a8-1703f4282787",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "session = get_active_session()\nimage=session.file.get_stream(\n    f\"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_image/{top_page_id}.png\",\n    decompress=False).read()\nst.image(image)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1ae6bea-b20c-42d5-9c8b-de1a63c0f859",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "Finally, we can also perform multimodal retrieval augmented generation (mRAG) by sending the query and the top page image to a multimodal LLM served on snowflake cortex and get a natural language answer to our question."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "SELECT SNOWFLAKE.CORTEX.COMPLETE('pixtral-large',\n    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL', 'paged_image/abbott-laboratories-10-q-2024-10-31_page_4.png',\n    'Answer the following question by referencing the document image: What was the overall operational cost incurred by Abbott Laboratories in 2023, and how much of this amount was allocated to research and development?');",
   "execution_count": null,
   "outputs": []
  }
 ]
}
