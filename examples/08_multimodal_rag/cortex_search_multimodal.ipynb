{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02b90c5-e06b-4082-ad0c-7e3a5e31a457",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell13"
   },
   "source": [
    "# Implementing multimodal retrieval using Cortex Search Service\n",
    "\n",
    "Welcome! This tutorial shows a lightweight example where a customer has 2 long pdfs and wants to search and ask natural questions on them. On a high level, this tutorial demonstrates:\n",
    "\n",
    "- Convert long PDF files to document screenshots (images).\n",
    "- (Optional but highly recommended) Run parse_document on PDFs for auxiliary text retrieval to further improve quality.\n",
    "- Embed document screenshots using EMBED_IMAGE_1024 (PrPr) which runs `voyage-multimodal-3` under the hood\n",
    "- Create a Cortex Search Service using multimodal embeddings and OCR text.\n",
    "- Retrieve top pages using Cortex Search.\n",
    "- Get natural language answer with multimodal RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30728acc-9579-4149-bcc5-b3241e5eac65",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell15"
   },
   "source": [
    "To start with, make sure you have PDFs stored under a stage. The two PDF files used in this demo can be found at https://drive.google.com/drive/folders/1bExhPiJlF9aNushnXeLLBR4m9EMaShHw?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505524cb-873d-425d-83e9-de8ec433b5e4",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "-- CREATE SCHEMA IF NOT EXISTS CORTEX_SEARCH_DB.PYU;\n",
    "-- CREATE OR REPLACE STAGE CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO\n",
    "-- STORAGE_INTEGRATION = ML_DEV\n",
    "-- URL = 's3://ml-dev-sfc-or-dev-misc1-k8s/cortexsearch/pyu/multimodal/demo/'\n",
    "-- DIRECTORY = (ENABLE = TRUE);\n",
    "\n",
    "-- CREATE OR REPLACE STAGE CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL DIRECTORY = (ENABLE = TRUE) ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');\n",
    "\n",
    "-- COPY FILES INTO @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\n",
    "-- FROM @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO/raw_pdf/;\n",
    "\n",
    "LS @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab710021-6fc0-4358-88eb-4bac717184c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell5"
   },
   "source": [
    "Now let's run some python code:\n",
    "\n",
    "The purpose is to paginate raw pages into pages -- in image and PDF format. Images are for multimodal retrieval, while PDFs are for better OCR quality (optional). As long as you configure the config correctly, you are good to go!\n",
    "\n",
    "```\n",
    "class Config:\n",
    "    input_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n",
    "    output_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/\"\n",
    "    input_path: str = \"raw_pdf\"\n",
    "    output_pdf_path: str = \"paged_pdf\"\n",
    "    output_image_path: str = \"paged_image\"\n",
    "    allowed_extensions: List[str] = None\n",
    "    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n",
    "    dpi: int = 300  # Default DPI for image conversion\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.allowed_extensions is None:\n",
    "            self.allowed_extensions = [\".pdf\"]\n",
    "```\n",
    "\n",
    "**Make sure the output_stage is an internal stage**, because `embed_image_1024` only works with internal stages at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import snowflake.snowpark.session as session\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def print_info(msg: str) -> None:\n",
    "    \"\"\"Print info message\"\"\"\n",
    "    print(f\"INFO: {msg}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def print_error(msg: str) -> None:\n",
    "    \"\"\"Print error message\"\"\"\n",
    "    print(f\"ERROR: {msg}\", file=sys.stderr)\n",
    "    if hasattr(st, \"error\"):\n",
    "        st.error(msg)\n",
    "\n",
    "\n",
    "def print_warning(msg: str) -> None:\n",
    "    \"\"\"Print warning message\"\"\"\n",
    "    print(f\"WARNING: {msg}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n",
    "    output_stage: str = (\n",
    "        \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/\"  # Base output stage without subdirectories\n",
    "    )\n",
    "    input_path: str = \"raw_pdf\"\n",
    "    output_pdf_path: str = \"paged_pdf\"\n",
    "    output_image_path: str = \"paged_image\"\n",
    "    allowed_extensions: List[str] = None\n",
    "    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n",
    "    dpi: int = 300  # Default DPI for image conversion\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.allowed_extensions is None:\n",
    "            self.allowed_extensions = [\".pdf\"]\n",
    "\n",
    "\n",
    "class PDFProcessingError(Exception):\n",
    "    \"\"\"Base exception for PDF processing errors\"\"\"\n",
    "\n",
    "\n",
    "class FileDownloadError(PDFProcessingError):\n",
    "    \"\"\"Raised when file download fails\"\"\"\n",
    "\n",
    "\n",
    "class PDFConversionError(PDFProcessingError):\n",
    "    \"\"\"Raised when PDF conversion fails\"\"\"\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def managed_temp_file(suffix: str = None) -> str:\n",
    "    \"\"\"Context manager for temporary file handling\"\"\"\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    try:\n",
    "        yield temp_file.name\n",
    "    finally:\n",
    "        # Don't delete the file immediately, let the caller handle cleanup\n",
    "        pass\n",
    "\n",
    "\n",
    "def cleanup_temp_file(file_path: str) -> None:\n",
    "    \"\"\"Clean up a temporary file\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except OSError as e:\n",
    "        print_warning(f\"Failed to delete temporary file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def list_pdf_files(session: session.Session, config: Config) -> List[dict]:\n",
    "    \"\"\"List all PDF files in the source stage\"\"\"\n",
    "    try:\n",
    "        # Use LIST command instead of DIRECTORY function\n",
    "        query = f\"\"\"\n",
    "        LIST {config.input_stage}\n",
    "        \"\"\"\n",
    "\n",
    "        file_list = session.sql(query).collect()\n",
    "\n",
    "        # Filter for PDF files\n",
    "        pdf_files = []\n",
    "        for file_info in file_list:\n",
    "            full_path = file_info[\"name\"]\n",
    "            # Extract just the filename from the full path\n",
    "            file_name = os.path.basename(full_path)\n",
    "\n",
    "            if any(\n",
    "                file_name.lower().endswith(ext) for ext in config.allowed_extensions\n",
    "            ):\n",
    "                pdf_files.append(\n",
    "                    {\n",
    "                        \"RELATIVE_PATH\": file_name,  # Use just the filename\n",
    "                        \"FULL_STAGE_PATH\": full_path,  # Use full path for download\n",
    "                        \"SIZE\": file_info[\"size\"] if \"size\" in file_info else 0,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print_info(f\"Found {len(pdf_files)} PDF files in the stage\")\n",
    "        return pdf_files\n",
    "    except Exception as e:\n",
    "        print_error(f\"Failed to list files: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def download_file_from_stage(\n",
    "    session: session.Session, file_path: str, config: Config\n",
    ") -> str:\n",
    "    \"\"\"Download a file from stage using session.file.get\"\"\"\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        # Ensure there are no double slashes in the path\n",
    "        stage_path = f\"{config.input_stage.rstrip('/')}/{file_path.lstrip('/')}\"\n",
    "\n",
    "        # Get the file from stage\n",
    "        get_result = session.file.get(stage_path, temp_dir)\n",
    "        if not get_result or get_result[0].status != \"DOWNLOADED\":\n",
    "            raise FileDownloadError(f\"Failed to download file: {file_path}\")\n",
    "\n",
    "        # Construct the local path where the file was downloaded\n",
    "        local_path = os.path.join(temp_dir, os.path.basename(file_path))\n",
    "        if not os.path.exists(local_path):\n",
    "            raise FileDownloadError(f\"Downloaded file not found at: {local_path}\")\n",
    "\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print_error(f\"Error downloading {file_path}: {e}\")\n",
    "        # Clean up the temporary directory\n",
    "        try:\n",
    "            import shutil\n",
    "\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except Exception as cleanup_error:\n",
    "            print_warning(f\"Failed to clean up temporary directory: {cleanup_error}\")\n",
    "        raise FileDownloadError(f\"Failed to download file: {e}\")\n",
    "\n",
    "\n",
    "def upload_file_to_stage(\n",
    "    session: session.Session, file_path: str, output_path: str, config: Config\n",
    ") -> str:\n",
    "    \"\"\"Upload file to the output stage\"\"\"\n",
    "    try:\n",
    "        # Get the directory and filename from the output path\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        base_name = os.path.basename(output_path)\n",
    "\n",
    "        # Create the full stage path with subdirectory\n",
    "        stage_path = f\"{config.output_stage.rstrip('/')}/{output_dir.lstrip('/')}\"\n",
    "\n",
    "        # Read the content of the original file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        # Create a new file with the correct name\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file_path = os.path.join(temp_dir, base_name)\n",
    "\n",
    "        # Write the content to the new file\n",
    "        with open(temp_file_path, \"wb\") as f:\n",
    "            f.write(file_content)\n",
    "\n",
    "        # Upload the file using session.file.put with compression disabled\n",
    "        put_result = session.file.put(\n",
    "            temp_file_path, stage_path, auto_compress=False, overwrite=True\n",
    "        )\n",
    "\n",
    "        # Check upload status\n",
    "        if not put_result or len(put_result) == 0:\n",
    "            raise Exception(f\"Failed to upload file: {base_name}\")\n",
    "\n",
    "        if put_result[0].status not in [\"UPLOADED\", \"SKIPPED\"]:\n",
    "            raise Exception(f\"Upload failed with status: {put_result[0].status}\")\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "        return f\"Successfully uploaded {base_name} to {stage_path}\"\n",
    "    except Exception as e:\n",
    "        print_error(f\"Error uploading file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_pdf_files(config: Config) -> None:\n",
    "    \"\"\"Main process to orchestrate the PDF splitting\"\"\"\n",
    "    try:\n",
    "        session = get_active_session()\n",
    "        pdf_files = list_pdf_files(session, config)\n",
    "\n",
    "        for file_info in pdf_files:\n",
    "            file_path = file_info[\"RELATIVE_PATH\"]\n",
    "            print_info(f\"Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                # Download the PDF file\n",
    "                local_pdf_path = download_file_from_stage(session, file_path, config)\n",
    "\n",
    "                # Get base filename without extension\n",
    "                base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "                # Extract individual PDF pages\n",
    "                with open(local_pdf_path, \"rb\") as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    num_pages = len(pdf_reader.pages)\n",
    "                    print_info(f\"Converting PDF to {num_pages} pages of PDFs\")\n",
    "\n",
    "                    for i in range(num_pages):\n",
    "                        page_num = i + 1\n",
    "                        s3_pdf_output_path = (\n",
    "                            f\"{config.output_pdf_path}/{base_name}_page_{page_num}.pdf\"\n",
    "                        )\n",
    "                        pdf_writer = PyPDF2.PdfWriter()\n",
    "                        pdf_writer.add_page(pdf_reader.pages[i])\n",
    "                        temp_file = tempfile.NamedTemporaryFile(\n",
    "                            delete=False, suffix=\".pdf\"\n",
    "                        )\n",
    "                        local_pdf_tmp_file_name = temp_file.name\n",
    "                        with open(local_pdf_tmp_file_name, \"wb\") as output_file:\n",
    "                            pdf_writer.write(output_file)\n",
    "                        \n",
    "                        upload_file_to_stage(\n",
    "                            session, local_pdf_tmp_file_name, s3_pdf_output_path, config\n",
    "                        )\n",
    "                        cleanup_temp_file(local_pdf_tmp_file_name)\n",
    "                            \n",
    "                # Convert PDF to images                \n",
    "                with pdfplumber.open(local_pdf_path) as pdf:\n",
    "                    print_info(f\"Converting PDF to {len(pdf.pages)} images\")\n",
    "                    for i, page in enumerate(pdf.pages):\n",
    "                        page_num = i + 1\n",
    "                        # Get page dimensions\n",
    "                        width = page.width\n",
    "                        height = page.height\n",
    "\n",
    "                        # Determine if scaling is needed\n",
    "                        max_dim = max(width, height)\n",
    "                        if max_dim > config.max_dimension:\n",
    "                            # Calculate scale factor to fit within max_dimension\n",
    "                            scale_factor = config.max_dimension / max_dim\n",
    "                            width = int(width * scale_factor)\n",
    "                            height = int(height * scale_factor)\n",
    "\n",
    "                        img = page.to_image(resolution=config.dpi)\n",
    "                        temp_file = tempfile.NamedTemporaryFile(\n",
    "                            delete=False, suffix=\".png\"\n",
    "                        )\n",
    "                        local_image_tmp_file_name = temp_file.name\n",
    "                        img.save(local_image_tmp_file_name)\n",
    "\n",
    "                        s3_image_output_path = (\n",
    "                            f\"{config.output_image_path}/{base_name}_page_{page_num}.png\"\n",
    "                        )\n",
    "                        \n",
    "                        upload_file_to_stage(\n",
    "                            session, local_image_tmp_file_name, s3_image_output_path, config\n",
    "                        )\n",
    "                        cleanup_temp_file(local_image_tmp_file_name)\n",
    "                        \n",
    "                # Clean up the original downloaded file\n",
    "                cleanup_temp_file(local_pdf_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print_error(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print_error(f\"Fatal error in process_pdf_files: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "config = Config(dpi=200)\n",
    "process_pdf_files(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7440dc-dcff-4219-8093-8d389b29b599",
   "metadata": {},
   "source": [
    "Check out one image and see if it's clear. If you can't read clearly, neural models won't either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9c845-6dff-4f18-8df5-a2a01c482d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "\n",
    "image=session.file.get_stream(\n",
    "     f\"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_image/abbott-laboratories-10-q-2024-10-31_page_1.png\",  # change to one image on your stage\n",
    "     decompress=False).read()\n",
    "st.image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8857644-dc84-4c19-b0a4-594d5e19586c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell2"
   },
   "source": [
    "Now let's start the multimodal embedding part! We first create an intermediate table that holds relative file names of images, and then call `SNOWFLAKE.CORTEX.embed_image_1024` to turn them into vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6854a8-8f72-45e3-936a-8b6b6f792bf8",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS AS\n",
    "SELECT\n",
    "    CONCAT('paged_image/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n",
    "    REGEXP_SUBSTR(metadata$filename, '_page_(\\d+)\\.', 1, 1, 'e')::INTEGER as PAGE_NUMBER,\n",
    "    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\n",
    "FROM\n",
    "    @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_image/\n",
    "GROUP BY 1, 2, 3\n",
    ";\n",
    "\n",
    "SELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc56ebb-45a4-4ded-b9ed-2d11b6054af1",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS AS\n",
    "SELECT FILE_NAME, STAGE_PREFIX, SNOWFLAKE.CORTEX.embed_image_1024('voyage-multimodal-3', '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL', FILE_NAME) AS IMAGE_VECTOR\n",
    "FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_IMAGE_CORPUS;\n",
    "\n",
    "\n",
    "SELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce0184-ea75-415c-9911-c70028a9a20b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell14"
   },
   "source": [
    "Similarly, we call `SNOWFLAKE.CORTEX.PARSE_DOCUMENT` to extract text from PDF pages. We discover that, although multimodal retrieval is powerful, augmenting it with text retrieval for keyword matching can bring quality improvement on certain types of search tasks/queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002eaac-2163-4f96-8a77-9c052e4381db",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_PDF_CORPUS AS\n",
    "SELECT\n",
    "    CONCAT('paged_pdf/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n",
    "    REGEXP_SUBSTR(metadata$filename, '_page_(\\d+)\\.', 1, 1, 'e')::INTEGER as PAGE_NUMBER,\n",
    "    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\n",
    "FROM\n",
    "    @CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_pdf/\n",
    "GROUP BY 1, 2, 3\n",
    ";\n",
    "\n",
    "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC AS\n",
    "    SELECT\n",
    "        FILE_NAME,\n",
    "        PAGE_NUMBER,\n",
    "        STAGE_PREFIX,\n",
    "        PARSE_JSON(TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "            '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL',\n",
    "            FILE_NAME,\n",
    "            {'mode': 'LAYOUT'}\n",
    "        ))):content AS PARSE_DOC_OUTPUT\n",
    "    FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_PDF_CORPUS\n",
    ";\n",
    "\n",
    "SELECT * FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e715c5-ccef-40dc-958c-c655d780f457",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell16"
   },
   "source": [
    "Now we join image vectors and texts into a single table, and create a Cortex Search service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6901c9-6563-46ff-8369-9234169a799a",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE CORTEX_SEARCH_DB.PYU.DEMO_SEC_JOINED_DATA AS\n",
    "SELECT\n",
    "    v.FILE_NAME,\n",
    "    v.PAGE_NUMBER,\n",
    "    v.IMAGE_VECTOR AS VECTOR_MAIN,\n",
    "    p.PARSE_DOC_OUTPUT AS TEXT\n",
    "FROM\n",
    "    CORTEX_SEARCH_DB.PYU.DEMO_SEC_VM3_VECTORS v\n",
    "JOIN\n",
    "    CORTEX_SEARCH_DB.PYU.DEMO_SEC_PARSE_DOC p\n",
    "ON\n",
    "    REGEXP_SUBSTR(v.FILE_NAME, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) = REGEXP_SUBSTR(p.FILE_NAME, 'paged_pdf/(.*)\\\\.pdf$', 1, 1, 'e', 1);\n",
    "\n",
    "\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE CORTEX_SEARCH_DB.PYU.DEMO_SEC_CORTEX_SEARCH_SERVICE\n",
    "  TEXT INDEXES TEXT\n",
    "  VECTOR INDEXES VECTOR_MAIN\n",
    "  WAREHOUSE='SEARCH_L'\n",
    "  TARGET_LAG='1 day'\n",
    "AS (\n",
    "    SELECT \n",
    "        TO_VARCHAR(TEXT) AS TEXT, \n",
    "        PAGE_NUMBER, \n",
    "        VECTOR_MAIN\n",
    "        IMAGE_FILEPATH\n",
    "    FROM CORTEX_SEARCH_DB.PYU.DEMO_SEC_JOINED_DATA\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78771232-88b1-4096-83fb-b4e233e548d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell17"
   },
   "source": [
    "Note that multimodal retrieval is not GA in cortex search, thus we cannot specify a multimodal embedding model when creating the service. Instead, we will embed queries directly with `SNOWFLAKE.CORTEX.EMBED_TEXT_1024` and call cortex search service with `experimental={'queryEmbedding': query_vector}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5611514-3ef0-45e9-a921-28eabe710893",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "\n",
    "demo_query_text = \"What was the overall operational cost incurred by Abbott Laboratories in 2023, and how much of this amount was allocated to research and development?\"\n",
    "sql_output = session.sql(f\"\"\"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_1024('voyage-multimodal-3', 'Represent the query for retrieving supporting documents:  {demo_query_text}')\"\"\").collect()\n",
    "query_vector = list(sql_output[0].asDict().values())[0]\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e908c4-aa91-40d2-943b-5f2934741373",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "\n",
    "\n",
    "root = Root(session)\n",
    "# fetch service\n",
    "my_service = (root\n",
    "  .databases[\"CORTEX_SEARCH_DB\"]\n",
    "  .schemas[\"PYU\"]\n",
    "  .cortex_search_services[\"DEMO_SEC_CORTEX_SEARCH_SERVICE\"]\n",
    ")\n",
    "\n",
    "# query service\n",
    "resp = my_service.search(\n",
    "  query=demo_query_text,\n",
    "  columns=[\"TEXT\", \"PAGE_NUMBER\", \"IMAGE_FILEPATH\"],\n",
    "  limit=5,\n",
    "  experimental={'queryEmbedding': query_vector}\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"rank {i + 1}: {resp.to_dict()['results'][i]['PAGE_NUMBER']}\")\n",
    "\n",
    "top_page_id = resp.to_dict()['results'][0]['PAGE_NUMBER']",
    "\n",
    "top_page_path = resp.to_dict()['results'][0]['IMAGE_FILEPATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448324b-c09d-4ed2-b449-94b0615b124b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell18"
   },
   "source": [
    "Let's see the top ranked page we found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93407e70-4657-472e-a6a8-1703f4282787",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "image=session.file.get_stream(\n",
    "    f\"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/paged_image/{top_page_id}.png\",\n",
    "    decompress=False).read()\n",
    "st.image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae6bea-b20c-42d5-9c8b-de1a63c0f859",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell19"
   },
   "source": [
    "Finally, we can also perform multimodal retrieval augmented generation (mRAG) by sending the query and the top page image to a multimodal LLM served on snowflake cortex and get a natural language answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell3"
   },
   "source": [
    "SELECT SNOWFLAKE.CORTEX.COMPLETE('pixtral-large',\n",
    "    '@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL', '{top_page_path}',\n",
    "    'Answer the following question by referencing the document image: What was the overall operational cost incurred by Abbott Laboratories in 2023, and how much of this amount was allocated to research and development?');"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "lastEditStatus": {
   "authorEmail": "puxuan.yu@snowflake.com",
   "authorId": "7316233358469",
   "authorName": "PYU",
   "lastEditTime": 1743811030949,
   "notebookId": "xw7ke2yiifgrxgxsyy2f",
   "sessionId": "e3312463-a075-4b08-a68c-4eb1627ec394"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
