{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b663d4-4e46-4a09-8e02-7e45f56aa3bd",
   "metadata": {
    "collapsed": false,
    "name": "intro"
   },
   "source": [
    "# Cortex Search Reranker Fine-tuning\n",
    "\n",
    "At the heart of every modern search system is a powerful two-stage process. First, retrieval acts like a speedy scout, rapidly sifting through millions of documents to gather a set of promising candidates. But this initial step prioritizes speed over perfect accuracy. That's where the **reranker** takes the spotlight. It's a more sophisticated model that closely examines the top candidates, intelligently reordering them to push the absolute best results to the top. This ensures we see the most relevant information first. However, a generic, \"one-size-fits-all\" reranker can often miss crucial nuances. In this tutorial, we'll discover how fine-tuning transforms a good reranker into a great one, delivering pinpoint accuracy for our specific needs.\n",
    "\n",
    "We will be using a complex search dataset called [TREC Clinical Trials](https://www.trec-cds.org/2021.html). This dataset represents an interesting and challenging case for fine-tuning because:\n",
    "\n",
    "- The search queries are paragraph-long patient case descriptions (PCDs), in contrast to keyword-based or single-sentence queries.\n",
    "- It contains a large amount of domain-specific language and terminology, on which general search models have limited expertise.\n",
    "- The notion of relevance differs significantly from that in general web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d4863-0231-4cc8-8d4a-88e99989bbba",
   "metadata": {
    "language": "python",
    "name": "install_python_pkg"
   },
   "outputs": [],
   "source": [
    "# install ir_datasets for downloading sample data\n",
    "!pip install ir_datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e46020-3ad8-48b0-aae3-4b54fded1d5e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "setup"
   },
   "outputs": [],
   "source": [
    "# most python libraries we need in this tutorial\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import datetime\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from snowflake.ml.modeling.distributors.pytorch import (\n",
    "    PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig,\n",
    ")\n",
    "from snowflake.ml.modeling.distributors.pytorch.context import get_context\n",
    "\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Print the current role, warehouse, and database/schema\n",
    "print(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ead9f-540e-4c17-86f6-c57d3c262c33",
   "metadata": {
    "collapsed": false,
    "name": "stage_1_data_prep"
   },
   "source": [
    "# Stage 1: Data Preparation\n",
    "\n",
    "In this stage, we will\n",
    "\n",
    "- Collect documents in the corpus and store in a Snowflake table (if not there already);\n",
    "- Create a cortex search service for evaluation and data mining (if not there already);\n",
    "- Collect some test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2231f0-0c9f-4556-9ec3-8e15fb4d5627",
   "metadata": {
    "language": "python",
    "name": "data_download"
   },
   "outputs": [],
   "source": [
    "# download data and store into a Snowflake table\n",
    "\n",
    "CORPUS_TABLE_NAME = \"TRECCT_DOCUMENT_CORPUS\"\n",
    "\n",
    "dataset = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\")\n",
    "corpus_rows = []\n",
    "for doc in tqdm(dataset.docs_iter()):  # iterate through the corpus\n",
    "    # document processing logic\n",
    "    docid = doc.doc_id\n",
    "    title = doc.title\n",
    "    condition = doc.condition\n",
    "    summary = doc.summary\n",
    "    detailed_description = doc.detailed_description\n",
    "    eligibility = doc.eligibility\n",
    "\n",
    "    # for simplicity we concatenate four fields together to form document texts\n",
    "    doc_text = f\"TITLE: {title}\\nELIGIBILITY: {eligibility}\\nSUMMARY: {summary}\\nDETAILED_DESCRIPTION: {detailed_description}\"\n",
    "    corpus_rows.append({\n",
    "        \"DOCID\": docid,\n",
    "        \"TEXT\": doc_text\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(corpus_rows)\n",
    "\n",
    "session.write_pandas(\n",
    "    df,\n",
    "    table_name=CORPUS_TABLE_NAME,\n",
    "    auto_create_table=True,   # infer column names/types from pandas\n",
    "    overwrite=True           # set True to replace the table contents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13f1d1-fc4b-4688-a7f9-607e2bca607b",
   "metadata": {
    "language": "sql",
    "name": "check_doc_success"
   },
   "outputs": [],
   "source": [
    "-- quickly check if all docs are uploaded successfully\n",
    "SELECT COUNT(*) FROM TRECCT_DOCUMENT_CORPUS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efe3fd-788c-4c54-ac0c-332b25b2cd4d",
   "metadata": {
    "language": "python",
    "name": "collect_test_queries"
   },
   "outputs": [],
   "source": [
    "# collect test queries and write to table\n",
    "# though we have test queries from two years (2021 and 2022), we only use 2022 ones for evaluation\n",
    "# queries from 2021 will be lightly used to adjusting the prompts for data generation and evaluation\n",
    "\n",
    "PROMPT_QUERIES_TABLE_NAME = \"TRECCT_QUERIES_FOR_PROMPT\"\n",
    "TEST_QUERIES_TABLE_NAME = \"TRECCT_TEST_QUERIES\"\n",
    "test_queries_2021, test_queries_2022 = [], []\n",
    "\n",
    "for query in ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\").queries_iter():\n",
    "    qid, query_text = query.query_id, query.text\n",
    "    qid = \"2021_\" + qid\n",
    "    test_queries_2021.append({\n",
    "        \"QID\": qid,\n",
    "        \"TEXT\": query_text\n",
    "    })\n",
    "\n",
    "for query in ir_datasets.load(\"clinicaltrials/2021/trec-ct-2022\").queries_iter():\n",
    "    qid, query_text = query.query_id, query.text\n",
    "    qid = \"2022_\" + qid\n",
    "    test_queries_2022.append({\n",
    "        \"QID\": qid,\n",
    "        \"TEXT\": query_text\n",
    "    })\n",
    "\n",
    "df_prompt = pd.DataFrame(test_queries_2021)\n",
    "df_test = pd.DataFrame(test_queries_2022)\n",
    "\n",
    "session.write_pandas(\n",
    "    df_prompt,\n",
    "    table_name=PROMPT_QUERIES_TABLE_NAME,\n",
    "    auto_create_table=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "session.write_pandas(\n",
    "    df_test,\n",
    "    table_name=TEST_QUERIES_TABLE_NAME,\n",
    "    auto_create_table=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(df_test)} test queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66019976-1d2c-4f05-888b-225924df0b08",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "create_css"
   },
   "outputs": [],
   "source": [
    "-- Let's also create a cortex search service \n",
    "-- for evaluation and to help us efficiently build finetuning data\n",
    "\n",
    "CREATE CORTEX SEARCH SERVICE IF NOT EXISTS CSS_TRECCT\n",
    "  ON TEXT\n",
    "  ATTRIBUTES DOCID, TEXT\n",
    "  WAREHOUSE = 'SEARCH_L'  -- replace this with your available warehouse\n",
    "  TARGET_LAG = '30 days'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0-8k'\n",
    "  INITIALIZE = ON_CREATE\n",
    "  AS SELECT * FROM TRECCT_DOCUMENT_CORPUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53599cd-f65d-4c40-b9b2-1e5212fc2653",
   "metadata": {
    "collapsed": false,
    "name": "stage_2_relevance"
   },
   "source": [
    "# Stage 2: Defining Relevance\n",
    "\n",
    "In this stage, we need to establish a robust, coherent and reusable concept of “relevance.” **This is arguably the most important step in customizing the search model for our needs.** The essence of finetuning lies in translating the concept of relevance into data for model training. If the concept is incorrect or unclear, we risk training the model in unwanted directions.\n",
    "\n",
    "Specifically, in this stage, we will:\n",
    "\n",
    "- Start with a scoring criterion;\n",
    "- Iterate on the scoring criterion until it aligns with our empirical expectations.\n",
    "\n",
    "*Note: this step is best accomplished by or under the consultance of domain experts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9bd56-a375-49e9-bbf6-2e08887a603e",
   "metadata": {
    "collapsed": false,
    "name": "how_to_test_relevance"
   },
   "source": [
    "If the search task is similar to generic web search, a good starting point of the scoring criterion can be found in Figure 1 of [this paper](https://arxiv.org/abs/2406.06519). We won't be using this criterion as the definition of relevance is intricate.\n",
    "\n",
    "Based on materials in [TREC Clinical Trials](https://www.trec-cds.org/2021.html), it's clear that there are three levels of relevance in this search task: eligible (2), excludes (1) and not relevant (0). A good scoring criterion must be interpretable **by both humans and our data agent** (in this case, an LLM). We can validate this by checking if the agent's relevance judgments align with those of domain experts.\n",
    "\n",
    "Now suppose we know from [here](https://trec.nist.gov/data/trials/qrels2021.txt) that for query 1 from the 2021 split, document NCT00526812 is judged as 'excludes' (1), whereas NCT00003466 is judged as 'not relevant' (0). Let's quickly test whether a scoring criterion works with data agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278abe1-572a-4b3c-8228-1a80b3f1dfa6",
   "metadata": {
    "language": "sql",
    "name": "score_crit_test"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    b.QID AS QID, \n",
    "    a.DOCID AS DOCID,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE('llama3.3-70b',\n",
    "'[Instruction]\n",
    "\n",
    "Determine how well the given patient case description (PCD) matches the given clinal trial description (CTD), based on the scoring criteria provided below. Output score must be wrapped in [Score] [/Score] tag pair for easy parsing. Explain the level of relevance in separate [EXP] [/EXP] tag pair.\n",
    "\n",
    "[Scoring Criteria] \n",
    "\n",
    "- Score 2 (eligible): Disease type/location matches trial intent. Patient meets all major eligibility criteria (age, relapse status, prior therapy limits, etc.). \n",
    "- Score 1 (conceptually relevant but excluded) The trial is truly for the same disease entity in the same anatomical context (e.g., brain anaplastic astrocytoma if patient has brain anaplastic astrocytoma). The trial treatment could apply in principle to the patient’s disease type, but the patient fails on technical eligibility restrictions, such as: Too many prior lines of therapy; Performance status or lab values outside range; Excluded comorbidity (e.g., uncontrolled hypertension, liver failure). In other words: same disease, same setting, but patient disqualified. \n",
    "- Score 0 (not relevant): The trial is not truly applicable to the patient’s disease setting, even if the words overlap. Examples: Trial requires intracranial astrocytoma, but patient’s is spinal astrocytoma; Trial is only for first relapse, but patient is on third-line or beyond and the mechanism is specific to early relapse; Trial is targeting related but different histologies (e.g., oligoastrocytoma only, glioblastoma only, pediatric setting); Trial uses a local delivery approach (e.g., stereotactic catheter infusion) that is anatomically impossible for the patient’s tumor site.\n",
    "\n",
    "[Input PCD]\n",
    "' || b.TEXT || '\n",
    "\n",
    "[Input CTD]\n",
    "' || a.TEXT) as llm_output,TRY_TO_NUMBER(\n",
    "      TRIM(\n",
    "        REGEXP_SUBSTR(\n",
    "          LLM_JUDGE_OUTPUT,\n",
    "          '\\\\[\\\\s*score\\\\s*\\\\][\\\\s\\\\S]*?(\\\\d+)[\\\\s\\\\S]*?\\\\[\\\\s*/\\\\s*score\\\\s*\\\\]',\n",
    "          1, 1, 'is', 1\n",
    "        )\n",
    "      )\n",
    "    ) AS SCORE,\n",
    "    TRIM(\n",
    "      REGEXP_SUBSTR(\n",
    "        REGEXP_REPLACE(llm_output, '.*\\\\[\\\\s*exp\\\\s*\\\\]', '', 1, 1, 'is'),\n",
    "        '([\\\\s\\\\S]*?)\\\\[\\\\s*/\\\\s*exp\\\\s*\\\\]', 1, 1, 'is', 1\n",
    "      )\n",
    "    ) AS EXPLANATION,\n",
    "    b.text as PCD,\n",
    "    a.text as CTD\n",
    "    from RAW_CORPUS_TREC_CT a\n",
    "    join TRECCT_QUERIES_FOR_PROMPT b\n",
    "    -- where a.DOCID = 'NCT00526812' and b.QID = '2021_1';  --> should be 1 mostly\n",
    "    where a.DOCID = 'NCT00003466' and b.QID = '2021_1';  --> should be 0 mostly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e343b7-f325-422f-ba49-7b9620a05b0c",
   "metadata": {
    "collapsed": false,
    "name": "score_crit_conclusion"
   },
   "source": [
    "It seems that the data agent can comprehend the scoring criterion quite accurately! Since we will be generating more data and evaluating search quality using this criterion repeatedly, it’s worthwhile to spend additional time refining it on more query–document samples until we achieve satisfactory performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df3ff5-923c-4e3f-9ce5-70ef8b0b6772",
   "metadata": {
    "collapsed": false,
    "name": "stage_3_data_gen"
   },
   "source": [
    "# Stage 3: Synthetic Data Generation\n",
    "\n",
    "Now it's time to take our refined criterion of relevance to synthetically generate search queries. Specifically, we will in this stage:\n",
    "\n",
    "- Sample 10K documents and generate 1 synthetic query from each;\n",
    "- Automatically annotate the top results searching those synthetic queries as well as our test queries;\n",
    "- Split synthetic data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b34709-7b7d-4c28-bf06-1ca659857870",
   "metadata": {
    "language": "sql",
    "name": "sample_docs"
   },
   "outputs": [],
   "source": [
    "-- To make queries *grounded*, typically we need to generate queries *based on* certain documents.\n",
    "-- Let's first randomly sample some documents:\n",
    "CREATE OR REPLACE TABLE TRECCT_DOCUMENT_CORPUS_RANDOM_10K AS \n",
    "SELECT * FROM TRECCT_DOCUMENT_CORPUS ORDER BY RANDOM() LIMIT 10000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363edb8f-f89c-4d7e-8932-bd6f0e6466a0",
   "metadata": {
    "language": "sql",
    "name": "query_gen_sample"
   },
   "outputs": [],
   "source": [
    "-- Before generating the whole 10K queries,\n",
    "-- let's first try a few samples and eye-ball if they are realistic; \n",
    "-- if not, adjust prompts accordingly\n",
    "\n",
    "SELECT\n",
    "    'TRAINQ_' || row_number() over (order by docid) as QID,\n",
    "    DOCID,\n",
    "    TEXT,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE('llama3.3-70b', \n",
    "'[Instruction]\n",
    "\n",
    "Given a clinical trial description (CTD), write a synthetic one-paragraph patient case description (PCD) that is deemed \"eligible\" (score 2) or \"conceptually relevant but excluded\" (score 1) according to the scoring criteria below. Follow the format of three sample PCDs I provide below. Output PCD must be wrapped in [PCD] [/PCD] tag pair for easy parsing. Explain the level of relevance in separate [EXP] [/EXP] tag pair.\n",
    "\n",
    "[Scoring Criteria] \n",
    "\n",
    "- Score 2 (eligible): Disease type/location matches trial intent. Patient meets all major eligibility criteria (age, relapse status, prior therapy limits, etc.). \n",
    "- Score 1 (conceptually relevant but excluded) The trial is truly for the same disease entity in the same anatomical context (e.g., brain anaplastic astrocytoma if patient has brain anaplastic astrocytoma). The trial treatment could apply in principle to the patient’s disease type, but the patient fails on technical eligibility restrictions, such as: Too many prior lines of therapy; Performance status or lab values outside range; Excluded comorbidity (e.g., uncontrolled hypertension, liver failure). In other words: same disease, same setting, but patient disqualified. \n",
    "- Score 0 (not relevant): The trial is not truly applicable to the patient’s disease setting, even if the words overlap. Examples: Trial requires intracranial astrocytoma, but patient’s is spinal astrocytoma; Trial is only for first relapse, but patient is on third-line or beyond and the mechanism is specific to early relapse; Trial is targeting related but different histologies (e.g., oligoastrocytoma only, glioblastoma only, pediatric setting); Trial uses a local delivery approach (e.g., stereotactic catheter infusion) that is anatomically impossible for the patient’s tumor site.\n",
    "\n",
    "[Sample PCD 1]\n",
    "\n",
    "Patient is a 55yo woman with h/o ESRD on HD and peritoneal dialysis who presented with watery, non bloody diarrhea and weakness. She has a history of 2 prior C diff infections, the most recent just 1 month ago. Recent antibx use in the last month on prior admission. Was also txd for Cdiff at that time for 14 d. course with po vanco. Pt was initially admitted to the ICU and was septic on pressors (levophed) until the morning of [**8-26**] with leukocytosis but no fever. C diff assay positive on admission, and pt had leukocytosis consistent with C diff. Patient was placed on Vanco po, Flagyl IV and Flagyl po initially, and when patient improved she was transitioned to Vanco oral and Flagyl oral on [**8-29**]. Patient was treated with Vanco for an extended course of 6 weeks given her recurrent C diff. Pt was also encouraged to take probiotics and to bleach her home when she was discharged.\n",
    "\n",
    "[Sample PCD 2]\n",
    "\n",
    "A 45-year-old woman was referred to the emergency department with abdominal pain lasting about 4 days accompanied by nausea and 2 episodes of vomiting. The pain is localized to the epigastric region and radiates to the right upper quadrant. The pain is worsening after eating fatty food. The patient experienced similar pain twice in the past year. Her past medical history is remarkable for hypercholesterolemia and two C/sections. She has 2 children, and she is menopausal. She doesn\\'t smoke, drink alcohol, or use illicit drugs. She is mildly febrile. Her BP is 150/85, HR 115, RR 15, T 38.2, SpO2 98% on RA. On palpation, she experiences epigastric tenderness and tenderness in the right upper quadrant without rebound. Bowel sounds are normal. Laboratory analysis is remarkable for elevated ESR and leukocytosis with a left shift. The ultrasound revealed several gallstones and biliary sludge. The largest gallstone is 0.7cm. Surgery consultation recommends elective cholecystectomy.\n",
    "\n",
    "[Sample PCD 3]\n",
    "\n",
    "The patient is a 33-year-old woman complained of fatigue, weight gain and abnormal spotting between menses. No hirsutism or nipple discharge was detected. Her BMI was 34. Her lab results were remarkable for high TSH level (13 mU/L) and low free T4 level (0.2 ng/dl). Her anti-TPO levels were extremely high (120 IU/ml). She was diagnosed with Hashimoto\\'s thyroiditis. Her aunt, brother and mother have the same disease. After starting 250 mcg Levothyroxine per day, her symptoms improved significantly and her periods are normal. She is still overweight with BMI of 31. Her most recent thyroid profile revealed all results except for anti-TPO within the normal range: TSH: 2.35 mU/L Free T4: 2.7 ng/dl Anti-TPO: 75 IU/ml\n",
    "\n",
    "[Input CTD]\n",
    "\n",
    "' || TEXT) as llm_output,\n",
    "REGEXP_SUBSTR(llm_output, '\\\\[pcd\\\\]([\\\\s\\\\S]*?)\\\\[/pcd\\\\]', 1, 1, 'ie') as QUERY,\n",
    "REGEXP_SUBSTR(llm_output, '\\\\[exp\\\\]([\\\\s\\\\S]*?)\\\\[/exp\\\\]', 1, 1, 'ie') as EXPLANATION,\n",
    "FROM TRECCT_DOCUMENT_CORPUS_RANDOM_10K \n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c78c5-1487-4d62-8cb2-3ff9a87d8e76",
   "metadata": {
    "language": "sql",
    "name": "query_gen_full"
   },
   "outputs": [],
   "source": [
    "-- Now we can go ahead and generate more synthetic queries and store them in a table\n",
    "\n",
    "CREATE OR REPLACE TABLE TRECCT_SYNTHETIC_QUERIES_RANDOM_10K AS\n",
    "    SELECT\n",
    "        'TRAINQ_' || row_number() over (order by docid) as QID,\n",
    "        DOCID,\n",
    "        TEXT,\n",
    "        SNOWFLAKE.CORTEX.COMPLETE('llama3.3-70b', \n",
    "'[Instruction]\n",
    "\n",
    "Given a clinical trial description (CTD), wrtie a synthetic one-paragraph patient case description (PCD) that is deemed \"eligible\" (score 2) or \"conceptually relevant but excluded\" (score 1) according to the scoring criteria below. Follow the format of three sample PCDs I provide below. Output PCD must be wrapped in [PCD] [/PCD] tag pair for easy parsing. Explain the level of relevance in separate [EXP] [/EXP] tag pair.\n",
    "\n",
    "[Scoring Criteria] \n",
    "\n",
    "- Score 2 (eligible): Disease type/location matches trial intent. Patient meets all major eligibility criteria (age, relapse status, prior therapy limits, etc.). \n",
    "- Score 1 (conceptually relevant but excluded) The trial is truly for the same disease entity in the same anatomical context (e.g., brain anaplastic astrocytoma if patient has brain anaplastic astrocytoma). The trial treatment could apply in principle to the patient’s disease type, but the patient fails on technical eligibility restrictions, such as: Too many prior lines of therapy; Performance status or lab values outside range; Excluded comorbidity (e.g., uncontrolled hypertension, liver failure). In other words: same disease, same setting, but patient disqualified. \n",
    "- Score 0 (not relevant): The trial is not truly applicable to the patient’s disease setting, even if the words overlap. Examples: Trial requires intracranial astrocytoma, but patient’s is spinal astrocytoma; Trial is only for first relapse, but patient is on third-line or beyond and the mechanism is specific to early relapse; Trial is targeting related but different histologies (e.g., oligoastrocytoma only, glioblastoma only, pediatric setting); Trial uses a local delivery approach (e.g., stereotactic catheter infusion) that is anatomically impossible for the patient’s tumor site.\n",
    "\n",
    "[Sample PCD 1]\n",
    "\n",
    "Patient is a 55yo woman with h/o ESRD on HD and peritoneal dialysis who presented with watery, non bloody diarrhea and weakness. She has a history of 2 prior C diff infections, the most recent just 1 month ago. Recent antibx use in the last month on prior admission. Was also txd for Cdiff at that time for 14 d. course with po vanco. Pt was initially admitted to the ICU and was septic on pressors (levophed) until the morning of [**8-26**] with leukocytosis but no fever. C diff assay positive on admission, and pt had leukocytosis consistent with C diff. Patient was placed on Vanco po, Flagyl IV and Flagyl po initially, and when patient improved she was transitioned to Vanco oral and Flagyl oral on [**8-29**]. Patient was treated with Vanco for an extended course of 6 weeks given her recurrent C diff. Pt was also encouraged to take probiotics and to bleach her home when she was discharged.\n",
    "\n",
    "[Sample PCD 2]\n",
    "\n",
    "A 45-year-old woman was referred to the emergency department with abdominal pain lasting about 4 days accompanied by nausea and 2 episodes of vomiting. The pain is localized to the epigastric region and radiates to the right upper quadrant. The pain is worsening after eating fatty food. The patient experienced similar pain twice in the past year. Her past medical history is remarkable for hypercholesterolemia and two C/sections. She has 2 children, and she is menopausal. She doesn\\'t smoke, drink alcohol, or use illicit drugs. She is mildly febrile. Her BP is 150/85, HR 115, RR 15, T 38.2, SpO2 98% on RA. On palpation, she experiences epigastric tenderness and tenderness in the right upper quadrant without rebound. Bowel sounds are normal. Laboratory analysis is remarkable for elevated ESR and leukocytosis with a left shift. The ultrasound revealed several gallstones and biliary sludge. The largest gallstone is 0.7cm. Surgery consultation recommends elective cholecystectomy.\n",
    "\n",
    "[Sample PCD 3]\n",
    "\n",
    "The patient is a 33-year-old woman complained of fatigue, weight gain and abnormal spotting between menses. No hirsutism or nipple discharge was detected. Her BMI was 34. Her lab results were remarkable for high TSH level (13 mU/L) and low free T4 level (0.2 ng/dl). Her anti-TPO levels were extremely high (120 IU/ml). She was diagnosed with Hashimoto\\'s thyroiditis. Her aunt, brother and mother have the same disease. After starting 250 mcg Levothyroxine per day, her symptoms improved significantly and her periods are normal. She is still overweight with BMI of 31. Her most recent thyroid profile revealed all results except for anti-TPO within the normal range: TSH: 2.35 mU/L Free T4: 2.7 ng/dl Anti-TPO: 75 IU/ml\n",
    "\n",
    "[Input CTD]\n",
    "\n",
    "' || TEXT) as llm_output,\n",
    "    REGEXP_SUBSTR(llm_output, '\\\\[pcd\\\\]([\\\\s\\\\S]*?)\\\\[/pcd\\\\]', 1, 1, 'ie') as QUERY,\n",
    "    REGEXP_SUBSTR(llm_output, '\\\\[exp\\\\]([\\\\s\\\\S]*?)\\\\[/exp\\\\]', 1, 1, 'ie') as EXPLANATION,\n",
    "    FROM TRECCT_DOCUMENT_CORPUS_RANDOM_10K\n",
    "    WHERE QUERY IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a6f48-dc46-4fad-8faf-f81ecdfc3ec9",
   "metadata": {
    "language": "sql",
    "name": "check_synthetic_queries"
   },
   "outputs": [],
   "source": [
    "-- Check how many queries we get.\n",
    "-- It's normal we get less than the number of inputs,\n",
    "-- because sometimes LLMs don't follow format exactly and we have to disgard them.\n",
    "SELECT COUNT(*) FROM TRECCT_SYNTHETIC_QUERIES_RANDOM_10K;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82891e57-0fd3-42b1-9262-2f1230b9397f",
   "metadata": {
    "collapsed": false,
    "name": "howto_doc_mining"
   },
   "source": [
    "When fine-tuning a search model, each training query requires both positive (relevant) and negative documents. A common assumption is that the document used to generate the query can serve as the positive example. However, this approach can lead to suboptimal results, as these query-document pairs are often trivially relevant and provide little learning value for the model.\n",
    "\n",
    "To create a more effective dataset, we will instead use Cortex Search to find seemingly relevant documents for our training queries. A data agent will then carefully label these documents as either relevant or not relevant. This process yields more challenging hard positives and crucial hard negatives, which are essential for building a robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185796fa-94b5-4dd6-9a72-74a05f593d46",
   "metadata": {
    "language": "sql",
    "name": "doc_mining"
   },
   "outputs": [],
   "source": [
    "-- We will use batch cortex search (https://docs.snowflake.com/LIMITEDACCESS/cortex-search/batch-cortex-search) to get top results for those queries more efficiently\n",
    "-- For 8k-10k queries on an CSS with ~300K documents, this will take under 5 minutes\n",
    "\n",
    "CREATE OR REPLACE TABLE CSS_TOP30_SYNTHETIC_QUERIES AS\n",
    "    SELECT\n",
    "        q.QID as QID,\n",
    "        r.DOCID as DOCID,\n",
    "        r.METADATA$RANK as RANK,\n",
    "        r.METADATA$REQUEST_ID\n",
    "    FROM TRECCT_SYNTHETIC_QUERIES_RANDOM_10K AS q,\n",
    "    LATERAL CORTEX_SEARCH_BATCH(\n",
    "        service_name => 'CORTEX_SEARCH_DB.PYU.CSS_TRECCT',\n",
    "        query => q.QUERY,\n",
    "        limit => 30\n",
    "    ) AS r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a32a1e-7c38-4600-b54a-11bc68c98bc9",
   "metadata": {
    "language": "sql",
    "name": "doc_labeling_with_llm"
   },
   "outputs": [],
   "source": [
    "-- Let's judge those results!\n",
    "-- Note that this step might take hours depending on the amount of data and compute traffic!\n",
    "\n",
    "CREATE or REPLACE TABLE CSS_TOP30_SYNTHETIC_QUERIES_JUDGED as\n",
    "SELECT\n",
    "    b.QID as QID,\n",
    "    b.DOCID as DOCID,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE('llama3.3-70b', '\n",
    "[Instruction]\n",
    "Determine how well the given patient case description (PCD) matches the given clinal trial description (CTD), based on the scoring criteria provided below. Output score must be wrapped in [Score] [/Score] tag pair for easy parsing. Explain the level of relevance in separate [EXP] [/EXP] tag pair.\n",
    "\n",
    "[Scoring Criteria]\n",
    "- Score 2 (eligible): Disease type/location matches trial intent. Patient meets all major eligibility criteria (age, relapse status, prior therapy limits, etc.).\n",
    "- Score 1 (conceptually relevant but excluded) The trial is truly for the same disease entity in the same anatomical context (e.g., brain astrocytoma if patient has brain anaplastic astrocytoma). The trial treatment could apply in principle to the patient’s disease type, but the patient fails on technical eligibility restrictions, such as: Too many prior lines of therapy; Performance status or lab values outside range; Excluded comorbidity (e.g., uncontrolled hypertension, liver failure). In other words: same disease, same setting, but patient disqualified.\n",
    "- Score 0 (not relevant): The trial is not truly applicable to the patient’s disease setting, even if the words overlap. Examples: Trial requires intracranial astrocytoma, but patient’s is spinal astrocytoma; Trial is only for first relapse, but patient is on third-line or beyond and the mechanism is specific to early relapse; Trial is targeting related but different histologies (e.g., oligoastrocytoma only, glioblastoma only, pediatric setting); Trial uses a local delivery approach (e.g., stereotactic catheter infusion) that is anatomically impossible for the patient’s tumor site.\n",
    "\n",
    "[Input PCD]\n",
    "' || c.QUERY || '\n",
    "\n",
    "[Input CTD]\n",
    "' || a.TEXT) as LLM_JUDGE_OUTPUT,\n",
    "    TRY_TO_NUMBER(\n",
    "      TRIM(\n",
    "        REGEXP_SUBSTR(\n",
    "          LLM_JUDGE_OUTPUT,\n",
    "          '\\\\[\\\\s*score\\\\s*\\\\][\\\\s\\\\S]*?(\\\\d+)[\\\\s\\\\S]*?\\\\[\\\\s*/\\\\s*score\\\\s*\\\\]',\n",
    "          1, 1, 'is', 1\n",
    "        )\n",
    "      )\n",
    "    ) AS SCORE,\n",
    "    TRIM(\n",
    "      REGEXP_SUBSTR(\n",
    "        REGEXP_REPLACE(LLM_JUDGE_OUTPUT, '.*\\\\[\\\\s*exp\\\\s*\\\\]', '', 1, 1, 'is'),\n",
    "        '([\\\\s\\\\S]*?)\\\\[\\\\s*/\\\\s*exp\\\\s*\\\\]', 1, 1, 'is', 1\n",
    "      )\n",
    "    ) AS EXPLANATION\n",
    "from TRECCT_DOCUMENT_CORPUS a\n",
    "join CSS_TOP30_SYNTHETIC_QUERIES b \n",
    "join TRECCT_SYNTHETIC_QUERIES_RANDOM_10K c on a.DOCID = b.DOCID and b.QID = c.QID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b0438-a9eb-4872-82fa-a5c545287397",
   "metadata": {
    "language": "sql",
    "name": "test_data_search"
   },
   "outputs": [],
   "source": [
    "-- Let's also label our test data (reusable).\n",
    "-- First we search top 100 per query using cortex search.\n",
    "CREATE OR REPLACE TABLE CSS_TOP100_TEST_QUERIES AS\n",
    "    SELECT\n",
    "        q.QID as QID,\n",
    "        r.DOCID as DOCID,\n",
    "        r.METADATA$RANK as RANK,\n",
    "        r.METADATA$REQUEST_ID  --> in case need for batch search debugging\n",
    "    FROM TRECCT_TEST_QUERIES AS q,\n",
    "    LATERAL CORTEX_SEARCH_BATCH(\n",
    "        service_name => 'CORTEX_SEARCH_DB.PYU.CSS_TRECCT',\n",
    "        query => q.TEXT,\n",
    "        limit => 100\n",
    "    ) AS r;\n",
    "\n",
    "-- Finally we label these using an LLM the same way we label training data.\n",
    "CREATE or REPLACE TABLE CSS_TOP100_TEST_QUERIES_JUDGED as\n",
    "SELECT\n",
    "    b.QID as QID,\n",
    "    b.DOCID as DOCID,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE('llama3.3-70b', '\n",
    "[Instruction]\n",
    "Determine how well the given patient case description (PCD) matches the given clinal trial description (CTD), based on the scoring criteria provided below. Output score must be wrapped in [Score] [/Score] tag pair for easy parsing. Explain the level of relevance in separate [EXP] [/EXP] tag pair.\n",
    "\n",
    "[Scoring Criteria]\n",
    "- Score 2 (eligible): Disease type/location matches trial intent. Patient meets all major eligibility criteria (age, relapse status, prior therapy limits, etc.).\n",
    "- Score 1 (conceptually relevant but excluded) The trial is truly for the same disease entity in the same anatomical context (e.g., brain astrocytoma if patient has brain anaplastic astrocytoma). The trial treatment could apply in principle to the patient’s disease type, but the patient fails on technical eligibility restrictions, such as: Too many prior lines of therapy; Performance status or lab values outside range; Excluded comorbidity (e.g., uncontrolled hypertension, liver failure). In other words: same disease, same setting, but patient disqualified.\n",
    "- Score 0 (not relevant): The trial is not truly applicable to the patient’s disease setting, even if the words overlap. Examples: Trial requires intracranial astrocytoma, but patient’s is spinal astrocytoma; Trial is only for first relapse, but patient is on third-line or beyond and the mechanism is specific to early relapse; Trial is targeting related but different histologies (e.g., oligoastrocytoma only, glioblastoma only, pediatric setting); Trial uses a local delivery approach (e.g., stereotactic catheter infusion) that is anatomically impossible for the patient’s tumor site.\n",
    "\n",
    "[Input PCD]\n",
    "' || c.TEXT || '\n",
    "\n",
    "[Input CTD]\n",
    "' || a.TEXT) as LLM_JUDGE_OUTPUT,\n",
    "    TRY_TO_NUMBER(\n",
    "      TRIM(\n",
    "        REGEXP_SUBSTR(\n",
    "          LLM_JUDGE_OUTPUT,\n",
    "          '\\\\[\\\\s*score\\\\s*\\\\][\\\\s\\\\S]*?(\\\\d+)[\\\\s\\\\S]*?\\\\[\\\\s*/\\\\s*score\\\\s*\\\\]',\n",
    "          1, 1, 'is', 1\n",
    "        )\n",
    "      )\n",
    "    ) AS SCORE,\n",
    "    TRIM(\n",
    "        REGEXP_SUBSTR(\n",
    "          LLM_JUDGE_OUTPUT,\n",
    "          '\\\\[\\\\s*exp\\\\s*\\\\]([\\\\s\\\\S]*?)\\\\[\\\\s*/\\\\s*exp\\\\s*\\\\]',\n",
    "          1, 1, 'is', 1\n",
    "        )\n",
    "      ) AS EXP_TEXT\n",
    "from TRECCT_DOCUMENT_CORPUS a\n",
    "join CSS_TOP100_TEST_QUERIES b \n",
    "join TRECCT_TEST_QUERIES c on a.DOCID = b.DOCID and b.QID = c.QID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f5e84-fc5d-450b-b166-9aaa5be485ad",
   "metadata": {
    "language": "python",
    "name": "test_baseline"
   },
   "outputs": [],
   "source": [
    "# Let's define an evaluation function see how cortex search without reranker perform!\n",
    "\n",
    "def compute_ndcg_at_k(session,\n",
    "                      test_table,\n",
    "                      judged_table=\"CSS_TOP100_TEST_QUERIES_JUDGED\",\n",
    "                      k=10):\n",
    "    \"\"\"\n",
    "    Compute nDCG@k per query and the mean across queries.\n",
    "\n",
    "    Definition:\n",
    "      - DCG@k: use the predicted top-k (from test_table ordered by RANK) with judged scores.\n",
    "      - IDCG@k: use all labeled documents for the query (from judged_table), sorted by SCORE desc, take top-k.\n",
    "      - Unlabeled documents are treated as SCORE=0 for DCG.\n",
    "\n",
    "    Returns:\n",
    "      mean_ndcg (float), ndcg_per_query (dict {qid: ndcg_at_k})\n",
    "    \"\"\"\n",
    "    # Predicted list (used for DCG)\n",
    "    test_df = (\n",
    "        session.table(test_table)\n",
    "        .select(\"QID\", \"DOCID\", \"RANK\")\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "    # All labels (used for IDCG and to supply scores to predicted docs)\n",
    "    judged_df = (\n",
    "        session.table(judged_table)\n",
    "        .select(\"QID\", \"DOCID\", \"SCORE\")\n",
    "        .to_pandas()\n",
    "    )\n",
    "    judged_df[\"SCORE\"] = pd.to_numeric(judged_df[\"SCORE\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    def _dcg(scores):\n",
    "        # Exponential gain: 2^rel - 1\n",
    "        return sum((2**rel - 1) / math.log2(i + 2) for i, rel in enumerate(scores))\n",
    "\n",
    "    # Precompute IDCG@k for each QID from all labeled docs\n",
    "    idcg_by_qid = {}\n",
    "    for qid, g in judged_df.groupby(\"QID\", sort=False):\n",
    "        ideal_scores = g[\"SCORE\"].sort_values(ascending=False).tolist()[:k]\n",
    "        idcg_by_qid[qid] = _dcg(ideal_scores) if ideal_scores else 0.0\n",
    "\n",
    "    # Merge predicted with labels (unlabeled -> 0)\n",
    "    merged = test_df.merge(judged_df, on=[\"QID\", \"DOCID\"], how=\"left\")\n",
    "    merged[\"SCORE\"] = merged[\"SCORE\"].fillna(0)\n",
    "\n",
    "    ndcg_per_query = {}\n",
    "    for qid, g in merged.groupby(\"QID\", sort=False):\n",
    "        # DCG@k from predicted top-k\n",
    "        topk_scores = g.sort_values(\"RANK\").head(k)[\"SCORE\"].tolist()\n",
    "        dcg = _dcg(topk_scores)\n",
    "        idcg = idcg_by_qid.get(qid, 0.0)\n",
    "        ndcg_per_query[qid] = (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "    # Mean across all queries in test_table (including those with no labels -> nDCG=0)\n",
    "    mean_ndcg = (sum(ndcg_per_query.values()) / len(ndcg_per_query)) if ndcg_per_query else 0.0\n",
    "    return mean_ndcg, ndcg_per_query\n",
    "\n",
    "mean_ndcg_for_css = compute_ndcg_at_k(session=session, test_table=\"CSS_TOP100_TEST_QUERIES\", k=10)[0]\n",
    "print(f\"Cortex Search (without reranker) nDCG@10 on test set: {mean_ndcg_for_css:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6d94c-7942-4d66-b3a8-381db2318e07",
   "metadata": {
    "collapsed": false,
    "name": "stage_4_train_eval"
   },
   "source": [
    "# Stage 4: Training and Evaluation\n",
    "\n",
    "Cortex Search offers a decent out-of-the-box solution for most search tasks on which it wasn't specifically optimized on. Let's see how much a fine-tuned reranker can help lift the search quality to another level!\n",
    "\n",
    "In this stage, we will:\n",
    "\n",
    "- Transform our generated training data into the format fit for Snowflake's distributed training framework (Ray);\n",
    "- Sample from training data for efficient demonstration (optional & for demonstration purpose only);\n",
    "- Run training job with multiple GPUs and store model weights to a stage;\n",
    "- Evaluate all checkpoints on our test data and pick the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d277f-8cd8-4c8a-8fa7-b406f371c7a2",
   "metadata": {
    "language": "sql",
    "name": "transform_data_1"
   },
   "outputs": [],
   "source": [
    "-- Here we convert our training data into format compatible with our distributed training framework\n",
    "\n",
    "-- Clean up\n",
    "DROP VIEW IF EXISTS TMP_VAL_QIDS;\n",
    "DROP VIEW IF EXISTS TMP_TRAIN_QIDS;\n",
    "DROP VIEW IF EXISTS RERANKER_VAL;\n",
    "DROP VIEW IF EXISTS RERANKER_TRAIN;\n",
    "\n",
    "-- 1) 50 queries for validation, rest for training\n",
    "CREATE TEMP VIEW TMP_VAL_QIDS AS\n",
    "SELECT QID\n",
    "FROM (\n",
    "  SELECT DISTINCT QID\n",
    "  FROM TRECCT_SYNTHETIC_QUERIES_RANDOM_10K\n",
    ")\n",
    "QUALIFY ROW_NUMBER() OVER (ORDER BY RANDOM()) <= 50;\n",
    "\n",
    "CREATE TEMP VIEW TMP_TRAIN_QIDS AS\n",
    "SELECT DISTINCT QID\n",
    "FROM TRECCT_SYNTHETIC_QUERIES_RANDOM_10K\n",
    "WHERE QID NOT IN (SELECT QID FROM TMP_VAL_QIDS);\n",
    "\n",
    "-- 2) Materialize compact 3-column views (fixed order)\n",
    "--    LABEL rule: SCORE >= 1 -> 1 else 0\n",
    "CREATE TEMP VIEW RERANKER_VAL AS\n",
    "SELECT\n",
    "  Q.QUERY  AS QUERY_TEXT,\n",
    "  D.TEXT   AS DOC_TEXT,\n",
    "  IFF(L.SCORE >= 1, 1, 0) AS LABEL\n",
    "FROM CSS_TOP30_SYNTHETIC_QUERIES_JUDGED L\n",
    "JOIN TMP_VAL_QIDS V ON L.QID = V.QID\n",
    "JOIN TRECCT_SYNTHETIC_QUERIES_RANDOM_10K Q ON L.QID = Q.QID\n",
    "JOIN TRECCT_DOCUMENT_CORPUS D ON L.DOCID = D.DOCID;\n",
    "\n",
    "CREATE TEMP VIEW RERANKER_TRAIN AS\n",
    "SELECT\n",
    "  Q.QUERY  AS QUERY_TEXT,\n",
    "  D.TEXT   AS DOC_TEXT,\n",
    "  IFF(L.SCORE >= 1, 1, 0) AS LABEL\n",
    "FROM CSS_TOP30_SYNTHETIC_QUERIES_JUDGED L\n",
    "JOIN TMP_TRAIN_QIDS T ON L.QID = T.QID\n",
    "JOIN TRECCT_SYNTHETIC_QUERIES_RANDOM_10K Q ON L.QID = Q.QID\n",
    "JOIN TRECCT_DOCUMENT_CORPUS D ON L.DOCID = D.DOCID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059253d3-f76f-4c5c-b0da-bdd1d501cf80",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "transform_data_2"
   },
   "outputs": [],
   "source": [
    "# [IMPORTANT] Here for demonstration purposes we only use 50K (less than 20%) of the whole training set\n",
    "# Feel free to remove this limit to unlock more gains\n",
    "train_df = session.table(\"RERANKER_TRAIN\").select(\"QUERY_TEXT\", \"DOC_TEXT\", \"LABEL\").limit(50000)\n",
    "val_df   = session.table(\"RERANKER_VAL\").select(\"QUERY_TEXT\", \"DOC_TEXT\", \"LABEL\")\n",
    "\n",
    "train_connector = ShardedDataConnector.from_dataframe(train_df)\n",
    "val_connector   = ShardedDataConnector.from_dataframe(val_df)\n",
    "\n",
    "# Schema constants (fixed)\n",
    "Q_COL = \"QUERY_TEXT\"\n",
    "D_COL = \"DOC_TEXT\"\n",
    "Y_COL = \"LABEL\"\n",
    "\n",
    "print(len(train_df.to_pandas()), len(val_df.to_pandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f7a6a-da5e-4d8b-8113-e264d5f7cd3c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_rerank_module"
   },
   "outputs": [],
   "source": [
    "# Define wrapper classes for registry and SPCS serving\n",
    "\n",
    "import tempfile\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "\n",
    "class RerankerModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps tokenizer + HF model.\n",
    "    - forward(queries, docs) -> list[float] for service inference\n",
    "    - encode(q_list, d_list) for training batches\n",
    "    - save(path) / load(path) to persist/load one cohesive artifact\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, max_length: int = 1024, device: Optional[torch.device] = None):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.max_length = int(max_length)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self._device = device or torch.device(\"cpu\")\n",
    "        self.to(self._device)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, queries: List[str], docs: List[str]) -> List[float]:\n",
    "        if len(queries) != len(docs):\n",
    "            raise ValueError(\"queries and docs must have the same length\")\n",
    "        if len(queries) == 0:\n",
    "            return []\n",
    "        self.model.eval()\n",
    "        scores: List[float] = []\n",
    "        bs = min(len(queries), 128)\n",
    "        if isinstance(queries, pd.Series):\n",
    "            queries = queries.to_list()\n",
    "        if isinstance(docs, pd.Series):\n",
    "            docs = docs.to_list()\n",
    "        for i in range(0, len(queries), bs):\n",
    "            enc = self.tokenizer(\n",
    "                queries[i:i+bs], docs[i:i+bs],\n",
    "                padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(self.model.device) for k, v in enc.items()}\n",
    "            logits = self.model(**enc).logits.view(-1)\n",
    "            scores.extend(logits.detach().cpu().tolist())\n",
    "        return scores\n",
    "\n",
    "    def encode(self, q_list: List[str], d_list: List[str]) -> dict:\n",
    "        enc = self.tokenizer(\n",
    "            q_list, d_list,\n",
    "            padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.to(self._device) for k, v in enc.items()}\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), os.path.join(path, \"weights.pt\"))\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        with open(os.path.join(path, \"model_config.json\"), \"w\") as f:\n",
    "            json.dump({\"model_name\": self.model_name, \"max_length\": self.max_length}, f)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, session: Session, stage_path: str, device: Optional[torch.device] = None) -> \"RerankerModule\":\n",
    "        \"\"\"\n",
    "        Loads the reranker module from a Snowflake stage.\n",
    "\n",
    "        Args:\n",
    "            session: The active Snowpark session object.\n",
    "            stage_path: The path to the model artifact folder in a Snowflake stage (e.g., '@~/.../final').\n",
    "            device: The torch device to load the model onto.\n",
    "        \n",
    "        Returns:\n",
    "            An instance of RerankerModule.\n",
    "        \"\"\"\n",
    "        with tempfile.TemporaryDirectory() as temp_dir: \n",
    "            session.file.get(stage_path, temp_dir)\n",
    "            with open(os.path.join(temp_dir, \"model_config.json\"), \"r\") as f:\n",
    "                cfg = json.load(f)\n",
    "            \n",
    "            obj = cls(cfg[\"model_name\"], int(cfg[\"max_length\"]), device=device)\n",
    "            obj.tokenizer = AutoTokenizer.from_pretrained(temp_dir)\n",
    "            state = torch.load(os.path.join(temp_dir, \"weights.pt\"), map_location=device or \"cpu\")\n",
    "            obj.model.load_state_dict(state)\n",
    "            obj.to(device or torch.device(\"cpu\"))\n",
    "            obj.model.eval()\n",
    "            return obj\n",
    "\n",
    "class RerankerCustomModel(custom_model.CustomModel):\n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        self.reranker = self.context[\"reranker\"]\n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def forward(self, input: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores = self.reranker.forward(\n",
    "            input[\"queries\"],\n",
    "            input[\"docs\"],\n",
    "        )\n",
    "        return pd.DataFrame({'scores': scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6775191-199a-45d2-a0d1-6d8cbb5fbacd",
   "metadata": {
    "language": "python",
    "name": "setup_training"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n",
    "dashboard_url = get_ray_dashboard_url('\"Cortex Search Reranker Finetuning\"')\n",
    "print(f\"Access the Ray Dashboard here: {dashboard_url}\")\n",
    "\n",
    "# A good set of default configs on a 4-GPU machine\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_name: str = \"BAAI/bge-reranker-v2-m3\"\n",
    "    max_length: int = 1024\n",
    "    batch_size: int = 4\n",
    "    num_epochs: int = 1\n",
    "    lr: float = 1e-4\n",
    "    warmup_ratio: float = 0.05\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    # Distributed resources\n",
    "    num_nodes: int = 1\n",
    "    num_workers_per_node: int = 4\n",
    "    num_gpus_per_worker: int = 1\n",
    "\n",
    "CFG = TrainConfig()\n",
    "\n",
    "CKPT_STAGE = \"@~/bge_reranker_ckpts\"  # replace this if necessary\n",
    "RUN_ID = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Find artifacts at {CKPT_STAGE}/run_{RUN_ID}\")\n",
    "\n",
    "def train_func():\n",
    "    # --- Required imports ---\n",
    "    import os\n",
    "    import torch\n",
    "    import time, datetime\n",
    "    import torch.distributed as dist\n",
    "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    from torch.utils.data import DataLoader\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "    # --- DDP INITIALIZATION ---\n",
    "    backend = \"nccl\" if torch.cuda.is_available() and CFG.num_gpus_per_worker > 0 else \"gloo\"\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    device = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() and CFG.num_gpus_per_worker > 0 else torch.device(\"cpu\")\n",
    "    dist.init_process_group(backend=backend)\n",
    "\n",
    "    ctx = get_context()\n",
    "    dmap = ctx.get_dataset_map()\n",
    "    model_dir = ctx.get_model_dir()\n",
    "    session = get_active_session()\n",
    "\n",
    "    train_shard = dmap[\"train\"].get_shard()\n",
    "    val_shard   = dmap.get(\"val\").get_shard() if dmap.get(\"val\") else None\n",
    "\n",
    "    # Replace to_torch_datapipe() with to_torch_dataset() and DataLoader\n",
    "    def make_dataloader(shard, shuffle, drop):\n",
    "        if shard is None:\n",
    "            return None\n",
    "        dataset = shard.to_torch_dataset() \n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=CFG.batch_size, \n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop, \n",
    "            num_workers=0 \n",
    "        )\n",
    "\n",
    "    train_dl = make_dataloader(train_shard, shuffle=False, drop=True)\n",
    "    val_dl   = make_dataloader(val_shard, shuffle=False, drop=False)\n",
    "\n",
    "    # --- Pre-count steps ---\n",
    "    total_steps_this_epoch = 0\n",
    "    \n",
    "    # Heler function\n",
    "    def make_count_dataloader(shard, drop):\n",
    "        if shard is None:\n",
    "            return None\n",
    "        dataset = shard.to_torch_dataset() \n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=CFG.batch_size, \n",
    "            shuffle=False,\n",
    "            drop_last=drop, # Keep drop_last=True for accurate batch counting\n",
    "            num_workers=0 \n",
    "        )\n",
    "\n",
    "    # Use a count_dl instance to iterate and count the batches.\n",
    "    count_dl = make_count_dataloader(train_shard, drop=True)\n",
    "\n",
    "    if count_dl is not None:\n",
    "        for _b in count_dl:\n",
    "            total_steps_this_epoch += 1\n",
    "\n",
    "    milestones = set()\n",
    "    if total_steps_this_epoch > 0:\n",
    "        for r in (0.2, 0.4, 0.6, 0.8, 1.0):\n",
    "            milestones.add(max(1, int(total_steps_this_epoch * r)))\n",
    "    \n",
    "    # --- Encapsulated model/tokenizer ---\n",
    "    reranker = RerankerModule(model_name=CFG.model_name, max_length=CFG.max_length, device=device)\n",
    "    \n",
    "    # Explicitly use device_ids in barrier\n",
    "    if dist.is_initialized():\n",
    "        if device.type == \"cuda\":\n",
    "            dist.barrier(device_ids=[device.index])\n",
    "        else:\n",
    "            dist.barrier()\n",
    "            \n",
    "    reranker.model = DDP(reranker.model, device_ids=[local_rank] if device.type == \"cuda\" else None)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(reranker.model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    total_steps_est = max(CFG.num_epochs * max(total_steps_this_epoch,1) // max(CFG.gradient_accumulation_steps,1), 1)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(CFG.warmup_ratio * total_steps_est),\n",
    "        num_training_steps=total_steps_est\n",
    "    )\n",
    "\n",
    "    Q_COL, D_COL, Y_COL = \"QUERY_TEXT\", \"DOC_TEXT\", \"LABEL\"\n",
    "\n",
    "    def encode_batch(batch):\n",
    "        q_list = list(batch[Q_COL])\n",
    "        d_list = list(batch[D_COL])\n",
    "        targets = batch[Y_COL].to(device=device, dtype=torch.float32).view(-1)\n",
    "    \n",
    "        enc = reranker.encode(q_list, d_list)\n",
    "        return enc, targets, len(q_list)\n",
    "\n",
    "    def evaluate_on_val():\n",
    "        if val_dl is None: \n",
    "            return {\"val_loss\": None, \"val_acc\": None}\n",
    "        reranker.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_n = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for b in val_dl: \n",
    "                inputs, targets, n = encode_batch(b)\n",
    "                logits = reranker.model(**inputs).logits.view(-1)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, targets)\n",
    "                total_loss += float(loss.item()) * n\n",
    "                total_n += n\n",
    "                preds = (logits > 0).long()\n",
    "                correct += (preds == targets.long()).sum().item()\n",
    "        reranker.model.train()\n",
    "        if total_n == 0:\n",
    "            return {\"val_loss\": None, \"val_acc\": None}\n",
    "        return {\"val_loss\": total_loss / total_n, \"val_acc\": correct / total_n}\n",
    "\n",
    "    # Enhanced synchronization in save_and_upload\n",
    "    def save_and_upload(tag: str, extra_metrics: dict):\n",
    "        if dist.is_initialized():\n",
    "            if device.type == \"cuda\":\n",
    "                dist.barrier(device_ids=[device.index])\n",
    "            else:\n",
    "                dist.barrier()\n",
    "\n",
    "        if dist.is_initialized() and dist.get_rank() != 0:\n",
    "            pass \n",
    "        else:\n",
    "            export_dir = os.path.join(model_dir, f\"export_{tag}\")\n",
    "            os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "            unwrapped_model = reranker.model.module if hasattr(reranker.model, \"module\") else reranker.model\n",
    "            torch.save(unwrapped_model.state_dict(), os.path.join(export_dir, \"weights.pt\"))\n",
    "            reranker.tokenizer.save_pretrained(export_dir)\n",
    "\n",
    "            with open(os.path.join(export_dir, \"model_config.json\"), \"w\") as f:\n",
    "                json.dump({\"model_name\": reranker.model_name, \"max_length\": reranker.max_length}, f)\n",
    "\n",
    "            meta = {\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"tag\": tag,\n",
    "                \"model_name\": CFG.model_name,\n",
    "                \"max_length\": CFG.max_length,\n",
    "                \"batch_size_per_gpu\": CFG.batch_size,\n",
    "                \"num_workers\": CFG.num_nodes * CFG.num_workers_per_node,\n",
    "                \"grad_accum\": CFG.gradient_accumulation_steps,\n",
    "                **(extra_metrics or {}),\n",
    "            }\n",
    "            with open(os.path.join(export_dir, \"metrics.json\"), \"w\") as f:\n",
    "                json.dump(meta, f)\n",
    "\n",
    "            stage_prefix = f\"{CKPT_STAGE}/run_{RUN_ID}/{tag}\"\n",
    "            for fn in os.listdir(export_dir):\n",
    "                p = os.path.join(export_dir, fn)\n",
    "                if os.path.isfile(p):\n",
    "                    session.file.put(f\"file://{p}\", stage_prefix, auto_compress=False, overwrite=True)\n",
    "\n",
    "        if dist.is_initialized():\n",
    "            if device.type == \"cuda\":\n",
    "                dist.barrier(device_ids=[device.index])\n",
    "            else:\n",
    "                dist.barrier()\n",
    "\n",
    "    # --- training loop ---\n",
    "    start_time = time.time()\n",
    "    print(f\"[Rank{dist.get_rank()}]: Training started!\")\n",
    "    global_step = 0\n",
    "    for _ in range(CFG.num_epochs):\n",
    "        if train_dl is not None:\n",
    "            step_in_epoch = 0\n",
    "            for batch_rows in train_dl:\n",
    "                inputs, targets, _ = encode_batch(batch_rows)\n",
    "                logits = reranker.model(**inputs).logits.view(-1)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, targets)\n",
    "                (loss / CFG.gradient_accumulation_steps).backward()\n",
    "\n",
    "                if (global_step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "\n",
    "                step_in_epoch += 1\n",
    "                global_step += 1\n",
    "\n",
    "                if step_in_epoch in milestones:\n",
    "                    tag = f\"step{global_step}\"\n",
    "                    eval_metrics = evaluate_on_val()\n",
    "                    time_elapsed = int(time.time() - start_time)\n",
    "                    print(f\"[Rank{dist.get_rank()}]: Glocal step {global_step}, time elapsed {datetime.timedelta(seconds=time_elapsed)}\")\n",
    "                    save_and_upload(tag, {\"train_loss\": float(loss.item()), **eval_metrics})\n",
    "\n",
    "    # final save + upload\n",
    "    if dist.is_initialized():\n",
    "        if device.type == \"cuda\":\n",
    "            dist.barrier(device_ids=[device.index])\n",
    "        else:\n",
    "            dist.barrier()\n",
    "            \n",
    "    save_and_upload(\"final\", evaluate_on_val())\n",
    "\n",
    "    # DDP finalize\n",
    "    if dist.is_initialized():\n",
    "        if device.type == \"cuda\":\n",
    "            dist.barrier(device_ids=[device.index]);\n",
    "        else:\n",
    "            dist.barrier()\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e04587-1fc8-46e8-b7e2-0c4f784c619f",
   "metadata": {
    "language": "python",
    "name": "launch_distributed_training"
   },
   "outputs": [],
   "source": [
    "# Launch actual training jobs!\n",
    "# Note: training 50K samples with default configs takes between 1-2 hours.\n",
    "\n",
    "scaling = PyTorchScalingConfig(\n",
    "    num_nodes=CFG.num_nodes,\n",
    "    num_workers_per_node=CFG.num_workers_per_node,\n",
    "    resource_requirements_per_worker=WorkerResourceConfig(\n",
    "        num_cpus=4,\n",
    "        num_gpus=CFG.num_gpus_per_worker,\n",
    "    ),\n",
    ")\n",
    "\n",
    "dist_trainer = PyTorchDistributor(train_func=train_func, scaling_config=scaling)\n",
    "\n",
    "# Assuming train_connector and val_connector are defined elsewhere\n",
    "result = dist_trainer.run(\n",
    "    dataset_map={\"train\": train_connector, \"val\": val_connector},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634a182-ef1e-452f-8a6a-3bb3a5a8a4b1",
   "metadata": {
    "language": "sql",
    "name": "check_artifacts"
   },
   "outputs": [],
   "source": [
    "-- See artifacts saved during training\n",
    "LIST @~/bge_reranker_ckpts/run_20251015_234022;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29b068-e7c8-480c-976b-f50472788d29",
   "metadata": {
    "language": "python",
    "name": "eval_functions"
   },
   "outputs": [],
   "source": [
    "def rerank_and_eval(\n",
    "    session,\n",
    "    qd_pairs_table: str,\n",
    "    document_text_table: str,\n",
    "    query_text_table: str,\n",
    "    judged_table: str,\n",
    "    reranker_module: RerankerModule,\n",
    "    device: str = \"cuda:0\",\n",
    "    batch_size: int = 256,\n",
    "    k: int = 10,\n",
    "    output_temp_table: str = \"TMP_RERANK_RESULTS\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Rerank candidate documents per query using a reranker model,\n",
    "    write results to a temp table (QID, DOCID, RANK),\n",
    "    then compute mean nDCG@k and per-query nDCG using compute_ndcg_at_k().\n",
    "    The temp table is dropped automatically at the end.\n",
    "\n",
    "    Returns:\n",
    "        mean_ndcg (float), ndcg_per_query (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare model and tokenizer\n",
    "    reranker_module.model.to(device)\n",
    "\n",
    "    # Join query-doc pairs with text\n",
    "    from snowflake.snowpark.functions import col\n",
    "\n",
    "    qd = session.table(qd_pairs_table).select(\"QID\", \"DOCID\")\n",
    "    qtbl = session.table(query_text_table).select(\"QID\", col(\"TEXT\").alias(\"QUERY_TEXT\"))\n",
    "    dtbl = session.table(document_text_table).select(\"DOCID\", col(\"TEXT\").alias(\"DOC_TEXT\"))\n",
    "\n",
    "    joined = (\n",
    "        qd.join(qtbl, qd[\"QID\"] == qtbl[\"QID\"])\n",
    "          .join(dtbl, qd[\"DOCID\"] == dtbl[\"DOCID\"])\n",
    "          .select(qd[\"QID\"].alias(\"QID\"), qd[\"DOCID\"].alias(\"DOCID\"), qtbl[\"QUERY_TEXT\"], dtbl[\"DOC_TEXT\"])\n",
    "    )\n",
    "\n",
    "    # Count total pairs for progress bar\n",
    "    total_pairs = joined.count()\n",
    "\n",
    "    # Stream scoring\n",
    "    results = []\n",
    "    def flush_batch(buf_qid, buf_docid, buf_query, buf_doc):\n",
    "        if not buf_qid:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            scores = reranker_module.forward(buf_query, buf_doc)\n",
    "        for qid, docid, s in zip(buf_qid, buf_docid, scores):\n",
    "            results.append((qid, docid, float(s)))\n",
    "        return len(buf_qid)\n",
    "\n",
    "    buf_qid, buf_docid, buf_query, buf_doc = [], [], [], []\n",
    "    with tqdm(total=total_pairs, desc=\"Scoring pairs\", dynamic_ncols=True, mininterval=0.5) as pbar:\n",
    "        for row in joined.to_local_iterator():\n",
    "            qid, docid, qtxt, dtxt = row[\"QID\"], row[\"DOCID\"], row[\"QUERY_TEXT\"], row[\"DOC_TEXT\"]\n",
    "            buf_qid.append(qid); buf_docid.append(docid); buf_query.append(qtxt); buf_doc.append(dtxt)\n",
    "            if len(buf_qid) >= batch_size:\n",
    "                n = flush_batch(buf_qid, buf_docid, buf_query, buf_doc)\n",
    "                pbar.update(n)\n",
    "                buf_qid.clear(); buf_docid.clear(); buf_query.clear(); buf_doc.clear()\n",
    "        # final flush\n",
    "        n = flush_batch(buf_qid, buf_docid, buf_query, buf_doc)\n",
    "        pbar.update(n)\n",
    "    # Rank per QID\n",
    "    df = pd.DataFrame(results, columns=[\"QID\", \"DOCID\", \"score\"])\n",
    "    df[\"RANK\"] = (\n",
    "        df.sort_values([\"QID\", \"score\"], ascending=[True, False])\n",
    "          .groupby(\"QID\")\n",
    "          .cumcount() + 1\n",
    "    )\n",
    "    out_df = pd.DataFrame({\n",
    "        \"QID\": df[\"QID\"],\n",
    "        \"DOCID\": df[\"DOCID\"].astype(str),\n",
    "        \"RANK\": df[\"RANK\"],\n",
    "    })\n",
    "    sf_out = session.create_dataframe(out_df)\n",
    "    sf_out.write.save_as_table(output_temp_table, mode=\"overwrite\", table_type=\"temporary\")\n",
    "\n",
    "    # Compute nDCG@k\n",
    "    mean_ndcg, ndcg_per_query = compute_ndcg_at_k(\n",
    "        session,\n",
    "        test_table=output_temp_table,\n",
    "        judged_table=judged_table,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    # Drop temp table\n",
    "    session.sql(f\"DROP TABLE IF EXISTS {output_temp_table}\").collect()\n",
    "\n",
    "    return mean_ndcg, ndcg_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538f04f-7479-4784-ae47-508092260498",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "eval_all_ckpts"
   },
   "outputs": [],
   "source": [
    "# evaluate all checkpoints!\n",
    "\n",
    "import altair as alt\n",
    "import streamlit as st\n",
    "\n",
    "eval_results = {}\n",
    "\n",
    "for steps in [0, 625, 1250, 1875, 2500, 3125]:\n",
    "    if steps == 0:\n",
    "        # un-tuned reranker\n",
    "        reranker = RerankerModule(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "    else:\n",
    "        checkpoint_path = f\"@~/bge_reranker_ckpts/run_20251015_234022/step{steps}/\"\n",
    "        reranker = RerankerModule.load(session=session, stage_path=checkpoint_path)\n",
    "        \n",
    "    \n",
    "    mean_ndcg, ndcg_per_query = rerank_and_eval(\n",
    "        session=session,\n",
    "        qd_pairs_table=\"CSS_TOP100_TEST_QUERIES\",\n",
    "        document_text_table=\"TRECCT_DOCUMENT_CORPUS\",\n",
    "        query_text_table=\"TRECCT_TEST_QUERIES\",\n",
    "        judged_table=\"CSS_TOP100_TEST_QUERIES_JUDGED\",\n",
    "        reranker_module=reranker,\n",
    "        device=\"cuda:0\",\n",
    "        batch_size=256,\n",
    "        k=10\n",
    "    )\n",
    "    print(f\"step {steps} nDCG@10: {mean_ndcg:.4f}\")\n",
    "    eval_results[steps] = (mean_ndcg, ndcg_per_query)\n",
    "\n",
    "\n",
    "\n",
    "mean_ndcg_vals = {k: v[0] for k, v in eval_results.items()}\n",
    "\n",
    "train_res_df = pd.DataFrame(\n",
    "    sorted(mean_ndcg_vals.items()),\n",
    "    columns=[\"STEP\", \"NDCG\"]\n",
    ")\n",
    "\n",
    "# Add the baseline line as an extra row for plotting convenience\n",
    "baseline_df = pd.DataFrame({\n",
    "    \"STEP\": train_res_df[\"STEP\"],\n",
    "    \"NDCG\": [mean_ndcg_for_css] * len(mean_ndcg_vals)\n",
    "})\n",
    "\n",
    "df_all = pd.concat([\n",
    "    train_res_df.assign(Type=\"Reranker\"),\n",
    "    baseline_df.assign(Type=\"No Reranking\")\n",
    "])\n",
    "\n",
    "ymin = max(0, mean_ndcg_for_css - 0.02)\n",
    "ymax = min(1, train_res_df[\"NDCG\"].max() + 0.02)\n",
    "\n",
    "st.subheader(\"nDCG@10 over Training Steps 📈\")\n",
    "st.caption(\"Tracking reranker performance vs. baseline (no reranking)\")\n",
    "\n",
    "# Build Altair chart with axis range\n",
    "chart = (\n",
    "    alt.Chart(df_all)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"STEP:Q\", title=\"Training Step\"),\n",
    "        y=alt.Y(\"NDCG:Q\", scale=alt.Scale(domain=[ymin, ymax]), title=\"Mean nDCG@10\"),\n",
    "        color=alt.Color(\"Type:N\", title=\"Legend\")\n",
    "    )\n",
    "    .properties(width=600, height=400)\n",
    ")\n",
    "\n",
    "st.altair_chart(chart, use_container_width=True)\n",
    "\n",
    "\n",
    "# Show numeric summary\n",
    "st.subheader(\"Summary 📊\")\n",
    "st.metric(\"Baseline -- no reranking\", f\"{mean_ndcg_for_css:.4f}\")\n",
    "st.metric(\"Baseline -- non-finetuned reranking\", f\"{train_res_df['NDCG'][0]:.4f}\")\n",
    "st.metric(\"Best Reranker nDCG\", f\"{train_res_df['NDCG'].max():.4f}\")\n",
    "st.metric(\"Best Reranker improvement\", f\"+{train_res_df['NDCG'].max()/train_res_df['NDCG'][0]-1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef961076-8f29-400f-9f3b-6a78cad7192b",
   "metadata": {
    "collapsed": false,
    "name": "stage_5_regis_service"
   },
   "source": [
    "# Stage 5: Register the model and create a service\n",
    "\n",
    "Now that we have a great reranker that improves significantly over no-reranking and a general-purpose reranker, in the final stage of the process, we can go ahead and register this model and create a service - so that we can use it repeatedly within or even outside Snowflake.\n",
    "\n",
    "*For demonstration purposes, we separate search (with Cortex Search) and reranking into two steps/service. Note that it's possible to bundle the new reranker into your Cortex Search Service*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42176d78-10fe-4663-8710-fe26debdada2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "register_model"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.registry.registry import Registry\n",
    "registry = Registry(session=session, database_name=f\"CORTEX_SEARCH_DB\", schema_name=\"PYU\")\n",
    "\n",
    "checkpoint_path = f\"@~/bge_reranker_ckpts/run_20251015_234022/final/\"\n",
    "reranker = RerankerModule.load(session=session, stage_path=checkpoint_path)\n",
    "\n",
    "reranker_custom_model = RerankerCustomModel(\n",
    "    context=custom_model.ModelContext(\n",
    "        reranker=reranker,\n",
    "    )\n",
    ")\n",
    "\n",
    "sample_input_data = pd.DataFrame(\n",
    "    {\n",
    "        \"queries\": [\"query1\", \"query1\"],\n",
    "        \"docs\": [\"doc1\", \"doc2\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "ref = registry.log_model(\n",
    "    model=reranker_custom_model,\n",
    "    model_name=\"bgem3_reranker\",\n",
    "    sample_input_data=sample_input_data,\n",
    "    pip_requirements=[\n",
    "        \"torch\",\n",
    "        \"transformers\",\n",
    "    ],\n",
    "    version_name=\"finetuned_trecct_50k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d942a4-601b-44fe-a10e-81c8a1520dbc",
   "metadata": {
    "language": "python",
    "name": "create_reranker_service"
   },
   "outputs": [],
   "source": [
    "ref.create_service(\n",
    "    service_name=\"bge_reranker_service_finetuned_50k\",\n",
    "    service_compute_pool=\"GPU_NV_S_COMPUTE_POOL\",  # Replace with a GPU compute pool you created\n",
    "    ingress_enabled=False,\n",
    "    gpu_requests=\"1\", # Model fits in GPU memory\n",
    "    max_instances=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282f8a7-bfc1-4c25-8df0-f3de81549ce2",
   "metadata": {
    "language": "sql",
    "name": "top10_results_original"
   },
   "outputs": [],
   "source": [
    "-- Let's review how relevant top-10 docs are without our reranker\n",
    "\n",
    "SELECT\n",
    "    a.QID,\n",
    "    a.DOCID,\n",
    "    a.RANK,\n",
    "    b.SCORE\n",
    "FROM\n",
    "    CSS_TOP100_TEST_QUERIES a\n",
    "JOIN\n",
    "    CSS_TOP100_TEST_QUERIES_JUDGED b ON a.QID = b.QID AND a.DOCID = b.DOCID\n",
    "WHERE\n",
    "    a.QID = '2022_5' AND a.RANK <= 10\n",
    "ORDER BY\n",
    "    a.RANK;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c5f2b-7940-4589-9f21-98c50a1049e0",
   "metadata": {
    "language": "python",
    "name": "top10_results_finetuned"
   },
   "outputs": [],
   "source": [
    "# Now let's see how the new reranker service makes the top-10 results much better! \n",
    "\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# get query text and document texts for test samples\n",
    "df_a = session.table('CSS_TOP100_TEST_QUERIES')\n",
    "df_b = session.table('TRECCT_TEST_QUERIES')\n",
    "df_c = session.table('TRECCT_DOCUMENT_CORPUS')\n",
    "\n",
    "# 2. Build the transformation plan with renamed columns\n",
    "final_df = df_a.join(df_b, df_a['qid'] == df_b['qid']) \\\n",
    "               .join(df_c, df_a['docid'] == df_c['docid']) \\\n",
    "               .where(df_a['qid'] == '2022_5') \\\n",
    "               .select(\n",
    "                   df_b['text'].alias('QUERY_TEXT'),\n",
    "                   df_c['docid'].alias('DOCID'),\n",
    "                   df_c['text'].alias('DOC_TEXT')\n",
    "               ).collect()\n",
    "\n",
    "full_pandas_df = pd.DataFrame(final_df)\n",
    "\n",
    "# transform into reranker format\n",
    "queries_docs_df = full_pandas_df[['QUERY_TEXT', 'DOC_TEXT']].rename(columns={'QUERY_TEXT': 'queries', 'DOC_TEXT': 'docs'})\n",
    "queries = full_pandas_df['QUERY_TEXT'].tolist()\n",
    "docs = full_pandas_df['DOC_TEXT'].tolist()\n",
    "docids_list = full_pandas_df['DOCID'].tolist()\n",
    "\n",
    "# send top 100 docs to the reranker service\n",
    "scores = ref.run(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"queries\": queries,\n",
    "            \"docs\": docs,\n",
    "        },\n",
    "    ),\n",
    "    function_name=\"forward\",\n",
    "    service_name=\"bge_reranker_service_finetuned_50k\"\n",
    ")\n",
    "\n",
    "# parse scores and display top 10 docs\n",
    "scores['DOCID'] = docids_list\n",
    "df_sorted = scores.sort_values(by='scores', ascending=False)\n",
    "top_10 = df_sorted.head(10).copy()\n",
    "top_10['RANK'] = range(1, 11)\n",
    "judge_df = session.table('CSS_TOP100_TEST_QUERIES_JUDGED')\n",
    "final_pandas_df = judge_df.join(session.create_dataframe(top_10), on='DOCID') \\\n",
    "                       .where(col('QID') == '2022_5') \\\n",
    "                       .select(\n",
    "                           col('QID'),\n",
    "                           col('DOCID'),\n",
    "                           col('RANK'),\n",
    "                           col('SCORE')\n",
    "                       ) \\\n",
    "                       .sort(col('RANK')) \\\n",
    "                       .to_pandas()\n",
    "\n",
    "final_pandas_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "puxuan.yu@snowflake.com",
   "authorId": "7316233358469",
   "authorName": "PYU",
   "lastEditTime": 1760732010151,
   "notebookId": "rycqop7decjfolf43yr5",
   "sessionId": "899326da-bd13-4854-a349-39dffdf7a0f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
