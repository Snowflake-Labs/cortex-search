{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.functions import col, udf\n",
    "from snowflake.snowpark.types import StringType, ArrayType, StructType, StructField, FloatType\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e877fca-d23e-4286-865e-55637bca904f",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "STRUCTURED_QUERY_PROMPT = \"\"\"You are an assitant helping a user searching for a product on amazon, given the user query and product information you have to give a rating. The user is flexible with product requirements which doesn't match the exact query. Specifically you should give a three point rating which is defined as:\n",
    "0 - Product doesn't seem related to the use query. \n",
    "1 - Product is related to the query and might vaguely satisfy the intent behind the query even though it might not match all the requirements present in the query and/or there are significant differences in sepcifications provided in the query (while still being related to the query).\n",
    "2 - Product is broadly what the user is searching for. However there might still be some minor differences in specifications, such as brand, or exact details of the product.\n",
    "3 - Product is completely what the user is searching for, matching all description in query. \n",
    "\n",
    "You should think step by step about the user query and the search result and rate the search result. You should also provide a reasoning for your rating.\n",
    "    \n",
    "Use the following format:\n",
    "Reasoning: Example Reasoning\n",
    "Rating: Example Rating\n",
    "\n",
    "### Examples\n",
    "Example 1:\n",
    "INPUT:\n",
    "Query: 4.2 Lt Freezerless Mini Fridge\n",
    "PRODUCT: \n",
    "TITLE: 4 Lt. Refrigerator Description: Energy Star Apartment Freezerless Fridge, Stainless Steel, E-Star with LED Lighting, Reversible Door, Adjustable Temperature, Quiet, for Dorm, Office, Home Kitchen\n",
    "OUTPUT:\n",
    "Reasoning: In this case the product is a 4 Lt. Refrigerator which is close to the 4.2 Lt Freezerless Mini Fridge and all other requirements match. Hence the product is a close match to the user query and is broadly what the user is looking for. Therefore it is rated 2.\n",
    "Rating: 2\n",
    "\n",
    "Example 2:\n",
    "INPUT:\n",
    "Query: Lead pencil without plastic grip\n",
    "PRODUCT: \n",
    "TITLE: Pencil Description: A pack of 10 lead pencils. The pencils are 2B and are perfect for writing and drawing. Erasers are provided for free with the 10 pencil pack.\n",
    "OUTPUT:\n",
    "Reasoning: The query mentions a lead pencil without a plastic grip. The product is a pack of 10 lead pencils which don't mention a plastic grip, so we can assume they don't have a plastic grip. Therefore the product is exactly what the user is looking for. Hence the rating is 2.\n",
    "Rating: 2\n",
    "\n",
    "Example 3:\n",
    "INPUT:\n",
    "Query: US immigration test pass gift cup\n",
    "PRODUCT: \n",
    "TITLE: Immigration Gift Jacket Description: A perfect gift for someone who recently pased the immigration test. US Flag is on the back of the jacket. \n",
    "OUTPUT:\n",
    "Reasoning: Although the query doesn't request for a jacket, it is about a gift for someone who passed immigration test. Hence the product vaguely satisfies the intent behind the query. Therefore the rating is 1.\n",
    "Rating: 1\n",
    "\n",
    "Example 4:\n",
    "INPUT:\n",
    "Query: Bomber Jacket with chinese collar\n",
    "PRODUCT: \n",
    "TITLE: Bomber Jacket Description: A stylish dark brown bomber jacket with a zipper and pockets with a fur collar. Will keep you warm in the winter while looking stylish. Needs dry cleaning.\n",
    "OUTPUT: \n",
    "Reasoning: The query is looking for a bomber jacket with chinese collars. The product is a bomber jacket with a fur collar. Which is a similar product to what the query is searching for. Therefore the rating is 1.\n",
    "Rating: 1\n",
    "\n",
    "Example 5:\n",
    "{example2_0}\n",
    "\n",
    "Example 6:\n",
    "{example2_1}\n",
    "\n",
    "Example 7:\n",
    "{example1_0}\n",
    "\n",
    "Example 8:\n",
    "{example1_1}\n",
    "\n",
    "Example 9:\n",
    "{example0_0}\n",
    "\n",
    "Example 10:\n",
    "{example3_0}\n",
    "###\n",
    "\n",
    "Now given the user query and search result below, rate the search result based on its relevance to the user query and provide a reasoning for your rating.\n",
    "INPUT:\n",
    "User Query: {query}\n",
    "Search Result: {passage}\n",
    "OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "RATINGS_TO_DEFAULT_EXAMPLES = {\n",
    "    \"0\": [\n",
    "\"\"\"Query: Daiwa Liberty Club Short Swing\n",
    "PRODUCT: YONEX AC1025P Tennis Badminton Grip. DESCRIPTION: Product Description Muscle Power locates the string on rounded archways that eliminate stress-load and fatigue through contact friction. \n",
    "OUTPUT:\n",
    "Rating: 0\n",
    "Reasoning: The query is looking for a fishing club short swing from Daiwa. But the product is a tennis badminton grip. It's completely unrelated.\n",
    "\"\"\",\n",
    "    ],\n",
    "    \"1\": [\n",
    "\"\"\"INPUT:\n",
    "Query: YONEX AC1025P Tennis Badminton Grip\n",
    "PRODUCT: TITLE: Yonex Badminton Racquet Voltric 200 Taufik Series - 80Gms DESCRIPTION: Product Description YONEX's head-light series, NANORAY provides a fast and controlled swing with enhanced repulsion via the New Aero Frame. \n",
    "OUTPUT:\n",
    "Rating: 1\n",
    "Reasoning: The query is looking for badminton grip, but product is badminton racquet instead. The product isn't what user is looking for.\n",
    "\"\"\",\n",
    "\"\"\"INPUT:\n",
    "User Query: 2010 dodge nitro crossbar\n",
    "PRODUCT: BLACK HORSE Armour Roll Bar Compatible with 2000 to 2022 Ram Chevrolet Ford GMC Toyota 3500 2500 Silverado F-150 Sierra Tundra 1500 2500 3500 Black Steel RB-AR1B\n",
    "OUTPUT:\n",
    "Rating: 1\n",
    "Reasoning: The query is looking for crossbar, but product is roll bar, which isn't same product. Even if dodge 2010 is mentioned, it's not a complete match given product doesn't align.\n",
    "\"\"\",\n",
    "         ],\n",
    "    \"2\": [\n",
    "\"\"\"INPUT:\n",
    "Query: Dogfish 500GB Msata Internal SSD\n",
    "PRODUCT: Crucial MX500 500GB 3D NAND SATA M.2 Internal SSD, up to 560MB/s & Seagate Barracuda 2TB Internal Hard Drive HDD – 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache 3.5-Inch – Frustration Free Packaging\n",
    "OUTPUT:\n",
    "Rating: 2\n",
    "Reasoning: Query is looking for 500GB SSD, which product satisfies. However, brand mentioned in query, Dogfish, doesn't match brand in product.\n",
    "\"\"\",\n",
    "\"\"\"INPUT:\n",
    "Query: YONEX AC1025P Tennis Badminton Grip\n",
    "PRODUCT: WILSON Pro Overgrip-Comfort DESCRIPTION: Product Description Will fit tennis, racquetball, badminton, and squash handles. Product Description Will fit tennis, racquetball, badminton, and squash handles.\n",
    "OUTPUT:\n",
    "Rating: 2\n",
    "Reasoning: The query is looking for badminton grip, which is aligned with product. But the product has minor details like brand that doens't fit query description.\n",
    "\"\"\",\n",
    "    ],\n",
    "    \"3\": [\n",
    "\"\"\"INPUT:\n",
    "Query: center console organizer\n",
    "PRODUCT: MX Auto Center Console Organizer| Compatible with Ford Trucks & SUVs – Accessories for F150, F250, F350, Raptor, Expedition|2015, 16, 17, 18, 19, 20, 21| Must-Have Bucket Seats|SEE COMPATIBILITY BELOW\n",
    "OUTPUT:\n",
    "Rating: 2\n",
    "Reasoning: Query and product are both center console organizer, complete match.\n",
    "\"\"\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def generate_llm_label(\n",
    "    input_query: str,\n",
    "    intermediate_columns: list[str],\n",
    "    output_select_columns: list[str],\n",
    "    output_table: str,\n",
    ") -> None:\n",
    "    table_raw = session.sql(input_query)\n",
    "    def generate_prompt(query: str, passage: str, golden_docs: list):\n",
    "        query_ratings = deepcopy(RATINGS_TO_DEFAULT_EXAMPLES)\n",
    "        ptrs = {\n",
    "            \"0\": len(RATINGS_TO_DEFAULT_EXAMPLES[\"0\"]), \n",
    "            \"1\": len(RATINGS_TO_DEFAULT_EXAMPLES[\"1\"]), \n",
    "            \"2\": len(RATINGS_TO_DEFAULT_EXAMPLES[\"2\"]), \n",
    "            \"3\": len(RATINGS_TO_DEFAULT_EXAMPLES[\"3\"]),\n",
    "        }\n",
    "        for gd in golden_docs:\n",
    "            gd_score = str(gd[\"score\"])\n",
    "            if gd_score in ptrs and ptrs[gd_score] > 0: # still need\n",
    "                # SKIPPED REASONING GIVEN NONE EXISTS, IT MAY NOT BE GOOD...\n",
    "                query_ratings[gd_score][ptrs[gd_score]-1] = f\"\"\"INPUT:\n",
    "Query: {query}\n",
    "PRODUCT: {gd[\"doc_text\"]}\n",
    "OUTPUT:\n",
    "Rating: {gd_score}\n",
    "\"\"\"\n",
    "        return STRUCTURED_QUERY_PROMPT.format(\n",
    "            query=query,\n",
    "            passage=passage,\n",
    "            example2_0=query_ratings[\"2\"][0],\n",
    "            example2_1=query_ratings[\"2\"][1],\n",
    "            example1_0=query_ratings[\"1\"][0],\n",
    "            example1_1=query_ratings[\"1\"][1],\n",
    "            example0_0=query_ratings[\"0\"][0],\n",
    "            example3_0=query_ratings[\"3\"][0],\n",
    "        )\n",
    "    \n",
    "    # Register the generate_prompt function as a UDF\n",
    "    golden_doc_struct = StructType([\n",
    "        StructField(\"score\", FloatType()),\n",
    "        StructField(\"doc_text\", StringType()),\n",
    "         StructField(\"doc_id\", StringType())\n",
    "    ])\n",
    "    input_cols = [StringType() for _ in range(len(intermediate_columns))]\n",
    "    input_cols.append(ArrayType(golden_doc_struct))\n",
    "    generate_prompt_udf = udf(\n",
    "        generate_prompt,\n",
    "        return_type=StringType(),\n",
    "        input_types=input_cols,\n",
    "        packages=['snowflake-snowpark-python'],\n",
    "        max_batch_size=100,\n",
    "    )\n",
    "    \n",
    "    # Apply the UDF to generate the 'generated_question' column\n",
    "    table_with_prompt = table_raw.with_column(\n",
    "        \"PROMPT\",\n",
    "        generate_prompt_udf(\n",
    "            *[col(colname) for colname in intermediate_columns],\n",
    "            col(\"GOLDEN_DOCS\"),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Filter and limit the rows, then show them\n",
    "    table_with_prompt = table_with_prompt.select(output_select_columns)\n",
    "    # table_with_prompt.limit(1).show()\n",
    "    \n",
    "    # Save the DataFrame to a Snowflake table\n",
    "    table_with_prompt.write.save_as_table(f\"{output_table}_INTERMEDIATE\", mode=\"overwrite\")\n",
    "\n",
    "    session.sql(f\"\"\"CREATE OR REPLACE TABLE {output_table} AS\n",
    "SELECT\n",
    "  *,\n",
    "  SNOWFLAKE.CORTEX.COMPLETE(\n",
    "      'llama3.1-405b',\n",
    "      [{{'role': 'user', 'content': prompt}}],\n",
    "      {{'temperature': 0,'top_p': 1}}\n",
    "  )['choices'][0]['messages']::VARCHAR AS LLM_JUDGE,\n",
    "  REGEXP_SUBSTR(LLM_JUDGE, 'Rating: ([0-9])', 1, 1, 'e', 1) AS LLM_RELEVANCE\n",
    "FROM {output_table}_INTERMEDIATE\"\"\").collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b320e-6df1-4b29-8741-7f5813cd2aab",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell5",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "input_query = \"\"\"SELECT QUERY_ID, TEXT, QUERY, RANK FROM CORTEX_SEARCH_DB.GOLDEN.TREC23_WITH_SUBTABLE_BASE_RESULTS\"\"\"\n",
    "\n",
    "generate_llm_label(\n",
    "    input_query=input_query,\n",
    "    intermediate_columns=[\"QUERY\", \"TEXT\"],\n",
    "    output_select_columns=[\"QUERY_ID\", \"QUERY\", \"TEXT\", \"PROMPT\", \"RANK\"],\n",
    "    output_table=\"CORTEX_SEARCH_DB.GOLDEN.TREC23_WITH_SUBTABLE_BASE_RESULTS_LLM_JUDGE\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
